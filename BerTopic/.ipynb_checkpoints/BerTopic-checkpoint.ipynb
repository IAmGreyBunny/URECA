{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0141ee-5d71-4b76-b6ed-e9dd004ee78f",
   "metadata": {},
   "source": [
    "# BerTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2519b-9b78-4f2f-8b44-4e122454a76d",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f390a36-418b-46d1-8bfa-4222f6d20a88",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b5f133-e723-4eb5-bfb9-65e6e09b76ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ea9b5-dd74-4db6-a1ee-4979a3ad55e1",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4906a23-e197-4567-9212-87e044e93a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"models\"\n",
    "DATASET_FOLDER = \"data\"\n",
    "MODEL_TRAINING_LOG = \"training_log.csv\"\n",
    "RESULT_FILE = \"result.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814009a-7594-4d5f-ae04-e4716652cd4d",
   "metadata": {},
   "source": [
    "### Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45b26b6-242e-4a7c-a2d4-5a8567672ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d8aaf-3451-408d-95ce-73b74f309146",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ec847-26c1-452f-83b7-e8e7e26373e2",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2f39fe2-df09-4574-b96b-3faec65295d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\URECA\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0269c8-0ae0-4e07-8469-f6874552532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"SetFit/20_newsgroups\")\n",
    "random.seed(42)\n",
    "text_label = list(zip(dataset[\"train\"][\"text\"], dataset[\"train\"][\"label_text\"]))\n",
    "sampled_text_label = random.sample(text_label, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b4473-3672-4732-9298-0ab83a5f262a",
   "metadata": {},
   "source": [
    "### Clean Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12bc7720-e7f8-4749-837a-fc8eded30b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_for_embedding(text, max_sentences=5):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith(\">\")]\n",
    "    lines = [line for line in lines if not re.match(r\"^\\s*(from|subject|organization|lines|writes|article)\\s*:\", line, re.IGNORECASE)]\n",
    "    text = \" \".join(lines)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[!?]{3,}\", \"\", text)\n",
    "    sentence_split = re.split(r'(?<=[.!?]) +', text)\n",
    "    sentence_split = [\n",
    "        s for s in sentence_split\n",
    "        if len(s.strip()) > 15 and not s.strip().isupper()\n",
    "      ]\n",
    "    return \" \".join(sentence_split[:max_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6d4dfa8-e6f8-4b7d-bac8-5275fc4ba388",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clean = [clean_for_embedding(text) for text,_ in sampled_text_label]\n",
    "labels = [label for _, label in sampled_text_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075daa7a-337d-45bc-9a5b-5f6247de0ba7",
   "metadata": {},
   "source": [
    "## 3. BerTopic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e53c0e39-9fd2-4331-8962-9bf79a481fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import torch\n",
    "\n",
    "def train_bertopic(embedding_model,n_neighbors,n_components,min_cluster_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_name = f\"{embedding_model}_{n_neighbors}_{n_components}_{min_cluster_size}\"\n",
    "\n",
    "    # Step 1 - Extract embeddings\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    embedding_model = SentenceTransformer(embedding_model,device)\n",
    "    \n",
    "    # Step 2 - Reduce dimensionality\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    \n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    \n",
    "    # Step 4 - Tokenize topics\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    \n",
    "    # Step 5 - Create topic representation\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "    \n",
    "    # Step 6 - (Optional) Fine-tune topic representations with\n",
    "    # a `bertopic.representation` model\n",
    "    representation_model = KeyBERTInspired()\n",
    "    \n",
    "    # All steps together\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model, # Step 1 - Extract embeddings\n",
    "        umap_model=umap_model, # Step 2 - Reduce dimensionality\n",
    "        hdbscan_model=hdbscan_model, # Step 3 - Cluster reduced embeddings\n",
    "        vectorizer_model=vectorizer_model, # Step 4 - Tokenize topics\n",
    "        ctfidf_model=ctfidf_model, # Step 5 - Extract topic words\n",
    "        representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(texts_clean)\n",
    "    \n",
    "    topic_model.save(f\"{MODEL_FOLDER}/{model_name}\", serialization=\"pytorch\")\n",
    "\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fc535-babf-4f49-8179-4024fd150e2f",
   "metadata": {},
   "source": [
    "## 4. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03083b60-6f21-4c58-890a-b3294aeeeb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = pd.DataFrame(columns=[\"model_name\",\"train_time\"])\n",
    "\n",
    "# Create Log file if it doesn't exist\n",
    "if not os.path.exists(MODEL_TRAINING_LOG):\n",
    "    trained_models = pd.DataFrame(columns=[\"model_name\",\"train_time\"])\n",
    "    trained_models.to_csv(MODEL_TRAINING_LOG, index=False)\n",
    "else:\n",
    "    trained_models = pd.read_csv(MODEL_TRAINING_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb93757-cc6e-438a-8174-7b94441b3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to test for\n",
    "embedding_models = [\"all-mpnet-base-v2\",\"all-MiniLM-L6-v2\"]\n",
    "n_neighbors_range = [x for x in range(5,21)]\n",
    "n_components_range = [x for x in range(5,21)]\n",
    "min_cluster_size_range = [x for x in range(5,26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f92a7c2-4e2b-48e9-8ac6-53d8f7d22d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-mpnet-base-v2_5_5_5 had already been trained\n",
      "all-mpnet-base-v2_5_5_6 had already been trained\n",
      "all-mpnet-base-v2_5_5_7 had already been trained\n",
      "all-mpnet-base-v2_5_5_8 had already been trained\n",
      "all-mpnet-base-v2_5_5_9 had already been trained\n",
      "all-mpnet-base-v2_5_5_10 had already been trained\n",
      "all-mpnet-base-v2_5_5_11 had already been trained\n",
      "all-mpnet-base-v2_5_5_12 had already been trained\n",
      "all-mpnet-base-v2_5_5_13 had already been trained\n",
      "all-mpnet-base-v2_5_5_14 had already been trained\n",
      "all-mpnet-base-v2_5_5_15 had already been trained\n",
      "all-mpnet-base-v2_5_5_16 had already been trained\n",
      "all-mpnet-base-v2_5_5_17 had already been trained\n",
      "all-mpnet-base-v2_5_5_18 had already been trained\n",
      "all-mpnet-base-v2_5_5_19 had already been trained\n",
      "all-mpnet-base-v2_5_5_20 had already been trained\n",
      "all-mpnet-base-v2_5_5_21 had already been trained\n",
      "all-mpnet-base-v2_5_5_22 had already been trained\n",
      "all-mpnet-base-v2_5_5_23 had already been trained\n",
      "all-mpnet-base-v2_5_5_24 had already been trained\n",
      "all-mpnet-base-v2_5_5_25 had already been trained\n",
      "all-mpnet-base-v2_5_6_5 had already been trained\n",
      "all-mpnet-base-v2_5_6_6 had already been trained\n",
      "all-mpnet-base-v2_5_6_7 had already been trained\n",
      "all-mpnet-base-v2_5_6_8 had already been trained\n",
      "all-mpnet-base-v2_5_6_9 had already been trained\n",
      "all-mpnet-base-v2_5_6_10 had already been trained\n",
      "all-mpnet-base-v2_5_6_11 had already been trained\n",
      "all-mpnet-base-v2_5_6_12 had already been trained\n",
      "all-mpnet-base-v2_5_6_13 had already been trained\n",
      "all-mpnet-base-v2_5_6_14 had already been trained\n",
      "all-mpnet-base-v2_5_6_15 had already been trained\n",
      "all-mpnet-base-v2_5_6_16 had already been trained\n",
      "all-mpnet-base-v2_5_6_17 had already been trained\n",
      "all-mpnet-base-v2_5_6_18 had already been trained\n",
      "all-mpnet-base-v2_5_6_19 had already been trained\n",
      "all-mpnet-base-v2_5_6_20 had already been trained\n",
      "all-mpnet-base-v2_5_6_21 had already been trained\n",
      "all-mpnet-base-v2_5_6_22 had already been trained\n",
      "all-mpnet-base-v2_5_6_23 had already been trained\n",
      "all-mpnet-base-v2_5_6_24 had already been trained\n",
      "all-mpnet-base-v2_5_6_25 had already been trained\n",
      "all-mpnet-base-v2_5_7_5 had already been trained\n",
      "all-mpnet-base-v2_5_7_6 had already been trained\n",
      "all-mpnet-base-v2_5_7_7 had already been trained\n",
      "all-mpnet-base-v2_5_7_8 had already been trained\n",
      "all-mpnet-base-v2_5_7_9 had already been trained\n",
      "all-mpnet-base-v2_5_7_10 had already been trained\n",
      "all-mpnet-base-v2_5_7_11 had already been trained\n",
      "all-mpnet-base-v2_5_7_12 had already been trained\n",
      "all-mpnet-base-v2_5_7_13 had already been trained\n",
      "all-mpnet-base-v2_5_7_14 had already been trained\n",
      "all-mpnet-base-v2_5_7_15 had already been trained\n",
      "all-mpnet-base-v2_5_7_16 had already been trained\n",
      "all-mpnet-base-v2_5_7_17 had already been trained\n",
      "all-mpnet-base-v2_5_7_18 had already been trained\n",
      "all-mpnet-base-v2_5_7_19 had already been trained\n",
      "all-mpnet-base-v2_5_7_20 had already been trained\n",
      "all-mpnet-base-v2_5_7_21 had already been trained\n",
      "all-mpnet-base-v2_5_7_22 had already been trained\n",
      "all-mpnet-base-v2_5_7_23 had already been trained\n",
      "all-mpnet-base-v2_5_7_24 had already been trained\n",
      "all-mpnet-base-v2_5_7_25 had already been trained\n",
      "all-mpnet-base-v2_5_8_5 had already been trained\n",
      "all-mpnet-base-v2_5_8_6 had already been trained\n",
      "all-mpnet-base-v2_5_8_7 had already been trained\n",
      "all-mpnet-base-v2_5_8_8 had already been trained\n",
      "all-mpnet-base-v2_5_8_9 had already been trained\n",
      "all-mpnet-base-v2_5_8_10 had already been trained\n",
      "all-mpnet-base-v2_5_8_11 had already been trained\n",
      "all-mpnet-base-v2_5_8_12 had already been trained\n",
      "all-mpnet-base-v2_5_8_13 had already been trained\n",
      "all-mpnet-base-v2_5_8_14 had already been trained\n",
      "all-mpnet-base-v2_5_8_15 had already been trained\n",
      "all-mpnet-base-v2_5_8_16 had already been trained\n",
      "all-mpnet-base-v2_5_8_17 had already been trained\n",
      "all-mpnet-base-v2_5_8_18 had already been trained\n",
      "all-mpnet-base-v2_5_8_19 had already been trained\n",
      "all-mpnet-base-v2_5_8_20 had already been trained\n",
      "all-mpnet-base-v2_5_8_21 had already been trained\n",
      "all-mpnet-base-v2_5_8_22 had already been trained\n",
      "all-mpnet-base-v2_5_8_23 had already been trained\n",
      "all-mpnet-base-v2_5_8_24 had already been trained\n",
      "all-mpnet-base-v2_5_8_25 had already been trained\n",
      "all-mpnet-base-v2_5_9_5 had already been trained\n",
      "all-mpnet-base-v2_5_9_6 had already been trained\n",
      "all-mpnet-base-v2_5_9_7 had already been trained\n",
      "all-mpnet-base-v2_5_9_8 had already been trained\n",
      "all-mpnet-base-v2_5_9_9 had already been trained\n",
      "all-mpnet-base-v2_5_9_10 had already been trained\n",
      "all-mpnet-base-v2_5_9_11 had already been trained\n",
      "all-mpnet-base-v2_5_9_12 had already been trained\n",
      "all-mpnet-base-v2_5_9_13 had already been trained\n",
      "all-mpnet-base-v2_5_9_14 had already been trained\n",
      "all-mpnet-base-v2_5_9_15 had already been trained\n",
      "all-mpnet-base-v2_5_9_16 had already been trained\n",
      "all-mpnet-base-v2_5_9_17 had already been trained\n",
      "all-mpnet-base-v2_5_9_18 had already been trained\n",
      "all-mpnet-base-v2_5_9_19 had already been trained\n",
      "all-mpnet-base-v2_5_9_20 had already been trained\n",
      "all-mpnet-base-v2_5_9_21 had already been trained\n",
      "all-mpnet-base-v2_5_9_22 had already been trained\n",
      "all-mpnet-base-v2_5_9_23 had already been trained\n",
      "all-mpnet-base-v2_5_9_24 had already been trained\n",
      "all-mpnet-base-v2_5_9_25 had already been trained\n",
      "all-mpnet-base-v2_5_10_5 had already been trained\n",
      "all-mpnet-base-v2_5_10_6 had already been trained\n",
      "all-mpnet-base-v2_5_10_7 had already been trained\n",
      "all-mpnet-base-v2_5_10_8 had already been trained\n",
      "all-mpnet-base-v2_5_10_9 had already been trained\n",
      "all-mpnet-base-v2_5_10_10 had already been trained\n",
      "all-mpnet-base-v2_5_10_11 had already been trained\n",
      "all-mpnet-base-v2_5_10_12 had already been trained\n",
      "all-mpnet-base-v2_5_10_13 had already been trained\n",
      "all-mpnet-base-v2_5_10_14 had already been trained\n",
      "all-mpnet-base-v2_5_10_15 had already been trained\n",
      "all-mpnet-base-v2_5_10_16 had already been trained\n",
      "all-mpnet-base-v2_5_10_17 had already been trained\n",
      "all-mpnet-base-v2_5_10_18 had already been trained\n",
      "all-mpnet-base-v2_5_10_19 had already been trained\n",
      "all-mpnet-base-v2_5_10_20 had already been trained\n",
      "all-mpnet-base-v2_5_10_21 had already been trained\n",
      "all-mpnet-base-v2_5_10_22 had already been trained\n",
      "all-mpnet-base-v2_5_10_23 had already been trained\n",
      "all-mpnet-base-v2_5_10_24 had already been trained\n",
      "all-mpnet-base-v2_5_10_25 had already been trained\n",
      "all-mpnet-base-v2_5_11_5 had already been trained\n",
      "all-mpnet-base-v2_5_11_6 had already been trained\n",
      "all-mpnet-base-v2_5_11_7 had already been trained\n",
      "all-mpnet-base-v2_5_11_8 had already been trained\n",
      "all-mpnet-base-v2_5_11_9 had already been trained\n",
      "all-mpnet-base-v2_5_11_10 had already been trained\n",
      "all-mpnet-base-v2_5_11_11 had already been trained\n",
      "all-mpnet-base-v2_5_11_12 had already been trained\n",
      "all-mpnet-base-v2_5_11_13 had already been trained\n",
      "all-mpnet-base-v2_5_11_14 had already been trained\n",
      "all-mpnet-base-v2_5_11_15 had already been trained\n",
      "all-mpnet-base-v2_5_11_16 had already been trained\n",
      "all-mpnet-base-v2_5_11_17 had already been trained\n",
      "all-mpnet-base-v2_5_11_18 had already been trained\n",
      "all-mpnet-base-v2_5_11_19 had already been trained\n",
      "all-mpnet-base-v2_5_11_20 had already been trained\n",
      "all-mpnet-base-v2_5_11_21 had already been trained\n",
      "all-mpnet-base-v2_5_11_22 had already been trained\n",
      "all-mpnet-base-v2_5_11_23 had already been trained\n",
      "all-mpnet-base-v2_5_11_24 had already been trained\n",
      "all-mpnet-base-v2_5_11_25 had already been trained\n",
      "all-mpnet-base-v2_5_12_5 had already been trained\n",
      "all-mpnet-base-v2_5_12_6 had already been trained\n",
      "all-mpnet-base-v2_5_12_7 had already been trained\n",
      "all-mpnet-base-v2_5_12_8 had already been trained\n",
      "all-mpnet-base-v2_5_12_9 had already been trained\n",
      "all-mpnet-base-v2_5_12_10 had already been trained\n",
      "all-mpnet-base-v2_5_12_11 had already been trained\n",
      "all-mpnet-base-v2_5_12_12 had already been trained\n",
      "all-mpnet-base-v2_5_12_13 had already been trained\n",
      "all-mpnet-base-v2_5_12_14 had already been trained\n",
      "all-mpnet-base-v2_5_12_15 had already been trained\n",
      "all-mpnet-base-v2_5_12_16 had already been trained\n",
      "all-mpnet-base-v2_5_12_17 had already been trained\n",
      "all-mpnet-base-v2_5_12_18 had already been trained\n",
      "all-mpnet-base-v2_5_12_19 had already been trained\n",
      "all-mpnet-base-v2_5_12_20 had already been trained\n",
      "all-mpnet-base-v2_5_12_21 had already been trained\n",
      "all-mpnet-base-v2_5_12_22 had already been trained\n",
      "all-mpnet-base-v2_5_12_23 had already been trained\n",
      "all-mpnet-base-v2_5_12_24 had already been trained\n",
      "all-mpnet-base-v2_5_12_25 had already been trained\n",
      "all-mpnet-base-v2_5_13_5 had already been trained\n",
      "all-mpnet-base-v2_5_13_6 had already been trained\n",
      "all-mpnet-base-v2_5_13_7 had already been trained\n",
      "all-mpnet-base-v2_5_13_8 had already been trained\n",
      "all-mpnet-base-v2_5_13_9 had already been trained\n",
      "all-mpnet-base-v2_5_13_10 had already been trained\n",
      "all-mpnet-base-v2_5_13_11 had already been trained\n",
      "all-mpnet-base-v2_5_13_12 had already been trained\n",
      "all-mpnet-base-v2_5_13_13 had already been trained\n",
      "all-mpnet-base-v2_5_13_14 had already been trained\n",
      "all-mpnet-base-v2_5_13_15 had already been trained\n",
      "all-mpnet-base-v2_5_13_16 had already been trained\n",
      "all-mpnet-base-v2_5_13_17 had already been trained\n",
      "all-mpnet-base-v2_5_13_18 had already been trained\n",
      "all-mpnet-base-v2_5_13_19 had already been trained\n",
      "all-mpnet-base-v2_5_13_20 had already been trained\n",
      "all-mpnet-base-v2_5_13_21 had already been trained\n",
      "all-mpnet-base-v2_5_13_22 had already been trained\n",
      "all-mpnet-base-v2_5_13_23 had already been trained\n",
      "all-mpnet-base-v2_5_13_24 had already been trained\n",
      "all-mpnet-base-v2_5_13_25 had already been trained\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_5 in 78.57062315940857\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_6 in 55.55042386054993\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_7 in 51.85754323005676\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_8 in 49.673030376434326\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_9 in 47.750635623931885\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_10 in 45.93761444091797\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_11 in 43.766345262527466\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_12 in 42.32611584663391\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_13 in 41.984089851379395\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_14 in 41.89123320579529\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_15 in 41.27368521690369\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_16 in 40.253440380096436\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_17 in 39.449936389923096\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_18 in 40.149518728256226\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_19 in 39.631115198135376\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_20 in 38.9762282371521\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_21 in 50.61107134819031\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_22 in 38.57186412811279\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_23 in 38.34242582321167\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_24 in 38.51798605918884\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_14_25 in 37.157490491867065\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_5 in 59.04212808609009\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_6 in 55.6203978061676\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_7 in 53.19058179855347\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_8 in 51.0704779624939\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_9 in 47.72291922569275\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_10 in 45.57339859008789\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_11 in 44.730730056762695\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_12 in 43.45418381690979\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_13 in 42.30589199066162\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_14 in 42.06179857254028\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_15 in 41.33318543434143\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_16 in 41.25409698486328\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_17 in 39.79749345779419\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_18 in 39.50101113319397\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_19 in 39.28081178665161\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_20 in 39.86818599700928\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_21 in 38.40025043487549\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_22 in 38.011173725128174\n",
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_15_23 in 38.53917741775513\n",
      "CUDA Available: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     10\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     model_name = \u001b[43mtrain_bertopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     end_time = time.time()\n\u001b[32m     13\u001b[39m     train_time = end_time-start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_bertopic\u001b[39m\u001b[34m(embedding_model, n_neighbors, n_components, min_cluster_size)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# All steps together\u001b[39;00m\n\u001b[32m     35\u001b[39m topic_model = BERTopic(\n\u001b[32m     36\u001b[39m     embedding_model=embedding_model, \u001b[38;5;66;03m# Step 1 - Extract embeddings\u001b[39;00m\n\u001b[32m     37\u001b[39m     umap_model=umap_model, \u001b[38;5;66;03m# Step 2 - Reduce dimensionality\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     representation_model=representation_model \u001b[38;5;66;03m# Step 6 - (Optional) Fine-tune topic representations\u001b[39;00m\n\u001b[32m     42\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m topics, probs = \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m topic_model.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, serialization=\u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:515\u001b[39m, in \u001b[36mBERTopic.fit_transform\u001b[39m\u001b[34m(self, documents, embeddings, images, y)\u001b[39m\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_representative_docs(custom_documents)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    514\u001b[39m     \u001b[38;5;66;03m# Extract topics by calculating c-TF-IDF, reduce topics if needed, and get representations.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_topics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnr_topics\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nr_topics:\n\u001b[32m    519\u001b[39m         documents = \u001b[38;5;28mself\u001b[39m._reduce_topics(documents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:4049\u001b[39m, in \u001b[36mBERTopic._extract_topics\u001b[39m\u001b[34m(self, documents, embeddings, mappings, verbose, fine_tune_representation)\u001b[39m\n\u001b[32m   4047\u001b[39m documents_per_topic = documents.groupby([\u001b[33m\"\u001b[39m\u001b[33mTopic\u001b[39m\u001b[33m\"\u001b[39m], as_index=\u001b[38;5;28;01mFalse\u001b[39;00m).agg({\u001b[33m\"\u001b[39m\u001b[33mDocument\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join})\n\u001b[32m   4048\u001b[39m \u001b[38;5;28mself\u001b[39m.c_tf_idf_, words = \u001b[38;5;28mself\u001b[39m._c_tf_idf(documents_per_topic)\n\u001b[32m-> \u001b[39m\u001b[32m4049\u001b[39m \u001b[38;5;28mself\u001b[39m.topic_representations_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_words_per_topic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcalculate_aspects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4054\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4055\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4056\u001b[39m \u001b[38;5;28mself\u001b[39m._create_topic_vectors(documents=documents, embeddings=embeddings, mappings=mappings)\n\u001b[32m   4058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:4369\u001b[39m, in \u001b[36mBERTopic._extract_words_per_topic\u001b[39m\u001b[34m(self, words, documents, c_tf_idf, fine_tune_representation, calculate_aspects, embeddings)\u001b[39m\n\u001b[32m   4367\u001b[39m         topics = tuner.extract_topics(\u001b[38;5;28mself\u001b[39m, documents, c_tf_idf, topics)\n\u001b[32m   4368\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fine_tune_representation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.representation_model, KeyBERTInspired):\n\u001b[32m-> \u001b[39m\u001b[32m4369\u001b[39m     topics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrepresentation_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_tf_idf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4370\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fine_tune_representation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.representation_model, BaseRepresentation):\n\u001b[32m   4371\u001b[39m     topics = \u001b[38;5;28mself\u001b[39m.representation_model.extract_topics(\u001b[38;5;28mself\u001b[39m, documents, c_tf_idf, topics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\representation\\_keybert.py:104\u001b[39m, in \u001b[36mKeyBERTInspired.extract_topics\u001b[39m\u001b[34m(self, topic_model, documents, c_tf_idf, topics, embeddings)\u001b[39m\n\u001b[32m    100\u001b[39m topics = \u001b[38;5;28mself\u001b[39m._extract_candidate_words(topic_model, c_tf_idf, topics)\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# We calculate the similarity between word and document embeddings and create\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# topic embeddings from the representative document embeddings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m sim_matrix, words = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentative_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_doc_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_embeddings\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Find the best matching words based on the similarity matrix for each topic\u001b[39;00m\n\u001b[32m    108\u001b[39m updated_topics = \u001b[38;5;28mself\u001b[39m._extract_top_words(words, topics, sim_matrix)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\representation\\_keybert.py:189\u001b[39m, in \u001b[36mKeyBERTInspired._extract_embeddings\u001b[39m\u001b[34m(self, topic_model, topics, representative_docs, repr_doc_indices, repr_embeddings)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Calculate word embeddings and extract best matching with updated topic_embeddings\u001b[39;00m\n\u001b[32m    188\u001b[39m vocab = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m topics.values() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]))\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m word_embeddings = \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocument\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m sim = cosine_similarity(topic_embeddings, word_embeddings)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sim, vocab\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:3729\u001b[39m, in \u001b[36mBERTopic._extract_embeddings\u001b[39m\u001b[34m(self, documents, images, method, verbose)\u001b[39m\n\u001b[32m   3727\u001b[39m     embeddings = \u001b[38;5;28mself\u001b[39m.embedding_model.embed_words(words=documents, verbose=verbose)\n\u001b[32m   3728\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3729\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3730\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m documents[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3731\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3732\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMake sure to use an embedding model that can either embed documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3733\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor images depending on which you want to embed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3734\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\backend\\_base.py:62\u001b[39m, in \u001b[36mBaseEmbedder.embed_documents\u001b[39m\u001b[34m(self, document, verbose)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, document: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> np.ndarray:\n\u001b[32m     51\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    matrix of embeddings.\u001b[39;00m\n\u001b[32m     53\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \u001b[33;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py:84\u001b[39m, in \u001b[36mSentenceTransformerBackend.embed\u001b[39m\u001b[34m(self, documents, verbose)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> np.ndarray:\n\u001b[32m     73\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m    matrix of embeddings.\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:486\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    485\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    495\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:338\u001b[39m, in \u001b[36mMPNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    336\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:297\u001b[39m, in \u001b[36mMPNetLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    289\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    290\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     **kwargs,\n\u001b[32m    296\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    305\u001b[39m     outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:238\u001b[39m, in \u001b[36mMPNetAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    230\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    231\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m     **kwargs,\n\u001b[32m    237\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.LayerNorm(\u001b[38;5;28mself\u001b[39m.dropout(self_outputs[\u001b[32m0\u001b[39m]) + hidden_states)\n\u001b[32m    246\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:174\u001b[39m, in \u001b[36mMPNetSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    167\u001b[39m v = (\n\u001b[32m    168\u001b[39m     \u001b[38;5;28mself\u001b[39m.v(hidden_states)\n\u001b[32m    169\u001b[39m     .view(batch_size, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_attention_heads, \u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    170\u001b[39m     .transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    171\u001b[39m )\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m attention_scores = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m attention_scores = attention_scores / math.sqrt(\u001b[38;5;28mself\u001b[39m.attention_head_size)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# Apply relative position embedding (precomputed in MPNetEncoder) if provided.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for embedding_model in embedding_models:\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "        for n_components in n_components_range:\n",
    "            for min_cluster_size in min_cluster_size_range:\n",
    "                model_name = f\"{embedding_model}_{n_neighbors}_{n_components}_{min_cluster_size}\"\n",
    "                if model_name in trained_models[\"model_name\"].values:\n",
    "                    print(f\"{model_name} had already been trained\")\n",
    "                    continue\n",
    "                else:\n",
    "                    start_time = time.time()\n",
    "                    model_name = train_bertopic(embedding_model,n_neighbors,n_components,min_cluster_size)\n",
    "                    end_time = time.time()\n",
    "                    train_time = end_time-start_time\n",
    "                    \n",
    "                    # Write to training log\n",
    "                    new_row = pd.DataFrame({\"model_name\": [model_name],\"train_time\":[train_time]})\n",
    "                    trained_models = pd.concat([trained_models, new_row], ignore_index=True)\n",
    "                    trained_models.to_csv(MODEL_TRAINING_LOG, index=False)\n",
    "                    \n",
    "                    # Print Status\n",
    "                    print(f\"Trained {model_name} in {train_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4125b8-9e2d-4bd9-a3d8-c3d1ca8d7c06",
   "metadata": {},
   "source": [
    "## 5. Result (Best Model) - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ee959-9d1c-423b-9c3d-5ce4a70de563",
   "metadata": {},
   "source": [
    "### Topic Info (Monogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e19e6-7e0c-4104-adb3-d79e6b736c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "monogram_topic_model = BERTopic.load(\"topic_model\",embedding_model=embedding_model)\n",
    "monogram_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc36926-f73b-4e6d-94f2-cb9f434236e1",
   "metadata": {},
   "source": [
    "### Topic Info (Multigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dcec9-f8f7-492d-9535-37e9c202853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multigram_topic_model = BERTopic.load(\"topic_model\",embedding_model=embedding_model)\n",
    "multigram_topic_model.update_topics(texts_clean, vectorizer_model=CountVectorizer(stop_words=\"english\", ngram_range=(2,3)))\n",
    "multigram_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3afba-bedb-4e22-a531-148e71e23202",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a63bc3-db86-4f32-8ee4-2de13c4c4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Tokenize Document\n",
    "tokenized_texts = [[str(token) for token in doc.split() if token.strip() != ''] for doc in texts_clean]\n",
    "\n",
    "# Create Dictionary\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Extract Topics\n",
    "# Filter topic words to exist in the dictionary\n",
    "topics = [\n",
    "    [str(word) for word, _ in words_probs if str(word) in dictionary.token2id]\n",
    "    for topic_id, words_probs in monogram_topic_model.get_topics().items()\n",
    "    if topic_id != -1\n",
    "]\n",
    "\n",
    "# Remove empty topics (just in case)\n",
    "topics = [t for t in topics if len(t) > 0]\n",
    "\n",
    "# Compute Coherence\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "monogram_coherence = coherence_model.get_coherence()\n",
    "print(\"Monogram C_v Coherence:\", monogram_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e1f9f-b717-42ef-8bf7-c6835d9beebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [doc.split() for doc in texts_clean]\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Topics have to be split into singular words\n",
    "topics = [\n",
    "    sum([word.split() for word, _ in multigram_topic_model.get_topic(topic)], [])\n",
    "    for topic in multigram_topic_model.get_topics().keys()\n",
    "    if topic != -1\n",
    "]\n",
    "\n",
    "# Remove empty topics (just in case)\n",
    "topics = [t for t in topics if len(t) > 0]\n",
    "\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "multigram_coherence = coherence_model.get_coherence()\n",
    "print(\"Multigram C_v Coherence:\", multigram_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8d8dd-4466-42b9-bc84-b0d62d2bec92",
   "metadata": {},
   "source": [
    "## 6. Using LLM to Improve Representation (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ab163-622c-44e3-94df-530c09acc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from bertopic.representation import OpenAI\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "topic_model.update_topics(texts_clean, representation_model=OpenAI(client, model=\"gpt-4o-mini\", delay_in_seconds=3))\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0344cd0-ca4e-41cd-825d-c31ac30c24c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
