{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0141ee-5d71-4b76-b6ed-e9dd004ee78f",
   "metadata": {},
   "source": [
    "# BerTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2519b-9b78-4f2f-8b44-4e122454a76d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f390a36-418b-46d1-8bfa-4222f6d20a88",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b5f133-e723-4eb5-bfb9-65e6e09b76ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ea9b5-dd74-4db6-a1ee-4979a3ad55e1",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4906a23-e197-4567-9212-87e044e93a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"models\"\n",
    "DATASET_FOLDER = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d8aaf-3451-408d-95ce-73b74f309146",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ec847-26c1-452f-83b7-e8e7e26373e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2f39fe2-df09-4574-b96b-3faec65295d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\URECA\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0269c8-0ae0-4e07-8469-f6874552532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"SetFit/20_newsgroups\")\n",
    "random.seed(42)\n",
    "text_label = list(zip(dataset[\"train\"][\"text\"], dataset[\"train\"][\"label_text\"]))\n",
    "sampled_text_label = random.sample(text_label, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b4473-3672-4732-9298-0ab83a5f262a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Clean Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12bc7720-e7f8-4749-837a-fc8eded30b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_for_embedding(text, max_sentences=5):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith(\">\")]\n",
    "    lines = [line for line in lines if not re.match(r\"^\\s*(from|subject|organization|lines|writes|article)\\s*:\", line, re.IGNORECASE)]\n",
    "    text = \" \".join(lines)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[!?]{3,}\", \"\", text)\n",
    "    sentence_split = re.split(r'(?<=[.!?]) +', text)\n",
    "    sentence_split = [\n",
    "        s for s in sentence_split\n",
    "        if len(s.strip()) > 15 and not s.strip().isupper()\n",
    "      ]\n",
    "    return \" \".join(sentence_split[:max_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4dfa8-e6f8-4b7d-bac8-5275fc4ba388",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clean = [clean_for_embedding(text) for text,_ in sampled_text_label]\n",
    "labels = [label for _, label in sampled_text_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075daa7a-337d-45bc-9a5b-5f6247de0ba7",
   "metadata": {},
   "source": [
    "## 3. BerTopic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53c0e39-9fd2-4331-8962-9bf79a481fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import torch\n",
    "\n",
    "def train_bertopic(embedding_model,n_neighbors,n_components,min_cluster_size):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model_name = f\"{embedding_model}_{n_neighbors}_{n_components}_{min_cluster_size}\"\n",
    "\n",
    "    # Step 1 - Extract embeddings\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    embedding_model = SentenceTransformer(\"all-mpnet-base-v2\",device)\n",
    "    \n",
    "    # Step 2 - Reduce dimensionality\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    \n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    \n",
    "    # Step 4 - Tokenize topics\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    \n",
    "    # Step 5 - Create topic representation\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "    \n",
    "    # Step 6 - (Optional) Fine-tune topic representations with\n",
    "    # a `bertopic.representation` model\n",
    "    representation_model = KeyBERTInspired()\n",
    "    \n",
    "    # All steps together\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model, # Step 1 - Extract embeddings\n",
    "        umap_model=umap_model, # Step 2 - Reduce dimensionality\n",
    "        hdbscan_model=hdbscan_model, # Step 3 - Cluster reduced embeddings\n",
    "        vectorizer_model=vectorizer_model, # Step 4 - Tokenize topics\n",
    "        ctfidf_model=ctfidf_model, # Step 5 - Extract topic words\n",
    "        representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(texts_clean)\n",
    "    \n",
    "    topic_model.save(f\"{MODEL_FOLDER}/{model_name}\", serialization=\"pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fc535-babf-4f49-8179-4024fd150e2f",
   "metadata": {},
   "source": [
    "## 4. Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4125b8-9e2d-4bd9-a3d8-c3d1ca8d7c06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. Result (Best Model) - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ee959-9d1c-423b-9c3d-5ce4a70de563",
   "metadata": {},
   "source": [
    "### Topic Info (Monogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e19e6-7e0c-4104-adb3-d79e6b736c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "monogram_topic_model = BERTopic.load(\"topic_model\",embedding_model=embedding_model)\n",
    "monogram_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc36926-f73b-4e6d-94f2-cb9f434236e1",
   "metadata": {},
   "source": [
    "### Topic Info (Multigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dcec9-f8f7-492d-9535-37e9c202853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multigram_topic_model = BERTopic.load(\"topic_model\",embedding_model=embedding_model)\n",
    "multigram_topic_model.update_topics(texts_clean, vectorizer_model=CountVectorizer(stop_words=\"english\", ngram_range=(2,3)))\n",
    "multigram_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3afba-bedb-4e22-a531-148e71e23202",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a63bc3-db86-4f32-8ee4-2de13c4c4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Tokenize Document\n",
    "tokenized_texts = [[str(token) for token in doc.split() if token.strip() != ''] for doc in texts_clean]\n",
    "\n",
    "# Create Dictionary\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Extract Topics\n",
    "# Filter topic words to exist in the dictionary\n",
    "topics = [\n",
    "    [str(word) for word, _ in words_probs if str(word) in dictionary.token2id]\n",
    "    for topic_id, words_probs in monogram_topic_model.get_topics().items()\n",
    "    if topic_id != -1\n",
    "]\n",
    "\n",
    "# Remove empty topics (just in case)\n",
    "topics = [t for t in topics if len(t) > 0]\n",
    "\n",
    "# Compute Coherence\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "monogram_coherence = coherence_model.get_coherence()\n",
    "print(\"Monogram C_v Coherence:\", monogram_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e1f9f-b717-42ef-8bf7-c6835d9beebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [doc.split() for doc in texts_clean]\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Topics have to be split into singular words\n",
    "topics = [\n",
    "    sum([word.split() for word, _ in multigram_topic_model.get_topic(topic)], [])\n",
    "    for topic in multigram_topic_model.get_topics().keys()\n",
    "    if topic != -1\n",
    "]\n",
    "\n",
    "# Remove empty topics (just in case)\n",
    "topics = [t for t in topics if len(t) > 0]\n",
    "\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "multigram_coherence = coherence_model.get_coherence()\n",
    "print(\"Multigram C_v Coherence:\", multigram_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8d8dd-4466-42b9-bc84-b0d62d2bec92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Using LLM to Improve Representation (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ab163-622c-44e3-94df-530c09acc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from bertopic.representation import OpenAI\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "topic_model.update_topics(texts_clean, representation_model=OpenAI(client, model=\"gpt-4o-mini\", delay_in_seconds=3))\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0344cd0-ca4e-41cd-825d-c31ac30c24c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
