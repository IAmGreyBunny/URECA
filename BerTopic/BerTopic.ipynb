{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0141ee-5d71-4b76-b6ed-e9dd004ee78f",
   "metadata": {},
   "source": [
    "# BerTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2519b-9b78-4f2f-8b44-4e122454a76d",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f390a36-418b-46d1-8bfa-4222f6d20a88",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b5f133-e723-4eb5-bfb9-65e6e09b76ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ea9b5-dd74-4db6-a1ee-4979a3ad55e1",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4906a23-e197-4567-9212-87e044e93a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"models\"\n",
    "BEST_MODEL_NAME = \"best_model\"\n",
    "DATASET_FOLDER = \"data\"\n",
    "TEST_DATA_FOLDER = \"test_data\"\n",
    "MODEL_TRAINING_LOG = \"training_log.csv\"\n",
    "MODEL_EVALUATION_LOG = \"eval_log.csv\"\n",
    "RESULT_FILE = \"result.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814009a-7594-4d5f-ae04-e4716652cd4d",
   "metadata": {},
   "source": [
    "### Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45b26b6-242e-4a7c-a2d4-5a8567672ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ea2ac-f5bb-4932-9c5f-aad0eddf9483",
   "metadata": {},
   "source": [
    "### Packages for Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdf2814-5f9c-40b5-a1dd-246d96e4a6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\URECA\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d6b4d-cad2-4a95-ab8f-d65854895112",
   "metadata": {},
   "source": [
    "### Packages for Gensim Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b2bca5-9e7e-4fe7-8095-5a75ef4ac4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d8aaf-3451-408d-95ce-73b74f309146",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ec847-26c1-452f-83b7-e8e7e26373e2",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f39fe2-df09-4574-b96b-3faec65295d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0269c8-0ae0-4e07-8469-f6874552532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"SetFit/20_newsgroups\")\n",
    "random.seed(42)\n",
    "text_label = list(zip(dataset[\"train\"][\"text\"], dataset[\"train\"][\"label_text\"]))\n",
    "sampled_text_label = random.sample(text_label, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b4473-3672-4732-9298-0ab83a5f262a",
   "metadata": {},
   "source": [
    "### Clean Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12bc7720-e7f8-4749-837a-fc8eded30b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_for_embedding(text, max_sentences=5):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith(\">\")]\n",
    "    lines = [line for line in lines if not re.match(r\"^\\s*(from|subject|organization|lines|writes|article)\\s*:\", line, re.IGNORECASE)]\n",
    "    text = \" \".join(lines)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[!?]{3,}\", \"\", text)\n",
    "    sentence_split = re.split(r'(?<=[.!?]) +', text)\n",
    "    sentence_split = [\n",
    "        s for s in sentence_split\n",
    "        if len(s.strip()) > 15 and not s.strip().isupper()\n",
    "      ]\n",
    "    return \" \".join(sentence_split[:max_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d4dfa8-e6f8-4b7d-bac8-5275fc4ba388",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clean = [clean_for_embedding(text) for text,_ in sampled_text_label]\n",
    "labels = [label for _, label in sampled_text_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075daa7a-337d-45bc-9a5b-5f6247de0ba7",
   "metadata": {},
   "source": [
    "## 3. BerTopic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e53c0e39-9fd2-4331-8962-9bf79a481fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bertopic(embedding_model_name,n_neighbors,n_components,min_cluster_size,embedding_model=None):\n",
    "    # Step 1 - Extract embeddings\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if embedding_model is None:\n",
    "        embedding_model = SentenceTransformer(embedding_model_name, device=DEVICE)\n",
    "\n",
    "    model_name = f\"{embedding_model_name}_{n_neighbors}_{n_components}_{min_cluster_size}\"\n",
    "    \n",
    "    # Step 2 - Reduce dimensionality\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    \n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    \n",
    "    # Step 4 - Tokenize topics\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    \n",
    "    # Step 5 - Create topic representation\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "    \n",
    "    # Step 6 - (Optional) Fine-tune topic representations with\n",
    "    # a `bertopic.representation` model\n",
    "    representation_model = KeyBERTInspired()\n",
    "    \n",
    "    # All steps together\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model, # Step 1 - Extract embeddings\n",
    "        umap_model=umap_model, # Step 2 - Reduce dimensionality\n",
    "        hdbscan_model=hdbscan_model, # Step 3 - Cluster reduced embeddings\n",
    "        vectorizer_model=vectorizer_model, # Step 4 - Tokenize topics\n",
    "        ctfidf_model=ctfidf_model, # Step 5 - Extract topic words\n",
    "        representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(texts_clean)\n",
    "    \n",
    "    topic_model.save(f\"{MODEL_FOLDER}/{model_name}\", serialization=\"pytorch\")\n",
    "\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fc535-babf-4f49-8179-4024fd150e2f",
   "metadata": {},
   "source": [
    "## 4. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03083b60-6f21-4c58-890a-b3294aeeeb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = pd.DataFrame(columns=[\"model_name\",\"train_time\"])\n",
    "\n",
    "# Create Log file if it doesn't exist\n",
    "if not os.path.exists(MODEL_TRAINING_LOG):\n",
    "    trained_models = pd.DataFrame(columns=[\"model_name\",\"train_time\"])\n",
    "    trained_models.to_csv(MODEL_TRAINING_LOG, index=False)\n",
    "else:\n",
    "    trained_models = pd.read_csv(MODEL_TRAINING_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ddb93757-cc6e-438a-8174-7b94441b3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to test for\n",
    "embedding_model_names = [\"all-mpnet-base-v2\",\"all-MiniLM-L6-v2\"]\n",
    "n_neighbors_range = [5, 10, 15, 25, 50]\n",
    "n_components_range = [5, 10, 15, 20, 25]\n",
    "min_cluster_size_range = [5, 10, 15, 20, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f92a7c2-4e2b-48e9-8ac6-53d8f7d22d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Trained all-mpnet-base-v2_5_5_5 in 53.18312072753906\n",
      "CUDA Available: True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     11\u001b[39m     start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     model_name = \u001b[43mtrain_bertopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_cluster_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     end_time = time.time()\n\u001b[32m     14\u001b[39m     train_time = end_time-start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mtrain_bertopic\u001b[39m\u001b[34m(embedding_model_name, n_neighbors, n_components, min_cluster_size, embedding_model)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# All steps together\u001b[39;00m\n\u001b[32m     26\u001b[39m topic_model = BERTopic(\n\u001b[32m     27\u001b[39m     embedding_model=embedding_model, \u001b[38;5;66;03m# Step 1 - Extract embeddings\u001b[39;00m\n\u001b[32m     28\u001b[39m     umap_model=umap_model, \u001b[38;5;66;03m# Step 2 - Reduce dimensionality\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     representation_model=representation_model \u001b[38;5;66;03m# Step 6 - (Optional) Fine-tune topic representations\u001b[39;00m\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m topics, probs = \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m topic_model.save(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_FOLDER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, serialization=\u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:515\u001b[39m, in \u001b[36mBERTopic.fit_transform\u001b[39m\u001b[34m(self, documents, embeddings, images, y)\u001b[39m\n\u001b[32m    511\u001b[39m     \u001b[38;5;28mself\u001b[39m._save_representative_docs(custom_documents)\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    514\u001b[39m     \u001b[38;5;66;03m# Extract topics by calculating c-TF-IDF, reduce topics if needed, and get representations.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_topics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnr_topics\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nr_topics:\n\u001b[32m    519\u001b[39m         documents = \u001b[38;5;28mself\u001b[39m._reduce_topics(documents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:4049\u001b[39m, in \u001b[36mBERTopic._extract_topics\u001b[39m\u001b[34m(self, documents, embeddings, mappings, verbose, fine_tune_representation)\u001b[39m\n\u001b[32m   4047\u001b[39m documents_per_topic = documents.groupby([\u001b[33m\"\u001b[39m\u001b[33mTopic\u001b[39m\u001b[33m\"\u001b[39m], as_index=\u001b[38;5;28;01mFalse\u001b[39;00m).agg({\u001b[33m\"\u001b[39m\u001b[33mDocument\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join})\n\u001b[32m   4048\u001b[39m \u001b[38;5;28mself\u001b[39m.c_tf_idf_, words = \u001b[38;5;28mself\u001b[39m._c_tf_idf(documents_per_topic)\n\u001b[32m-> \u001b[39m\u001b[32m4049\u001b[39m \u001b[38;5;28mself\u001b[39m.topic_representations_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_words_per_topic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4053\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcalculate_aspects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfine_tune_representation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4054\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4055\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4056\u001b[39m \u001b[38;5;28mself\u001b[39m._create_topic_vectors(documents=documents, embeddings=embeddings, mappings=mappings)\n\u001b[32m   4058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:4369\u001b[39m, in \u001b[36mBERTopic._extract_words_per_topic\u001b[39m\u001b[34m(self, words, documents, c_tf_idf, fine_tune_representation, calculate_aspects, embeddings)\u001b[39m\n\u001b[32m   4367\u001b[39m         topics = tuner.extract_topics(\u001b[38;5;28mself\u001b[39m, documents, c_tf_idf, topics)\n\u001b[32m   4368\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fine_tune_representation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.representation_model, KeyBERTInspired):\n\u001b[32m-> \u001b[39m\u001b[32m4369\u001b[39m     topics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrepresentation_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_tf_idf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4370\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m fine_tune_representation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.representation_model, BaseRepresentation):\n\u001b[32m   4371\u001b[39m     topics = \u001b[38;5;28mself\u001b[39m.representation_model.extract_topics(\u001b[38;5;28mself\u001b[39m, documents, c_tf_idf, topics)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\representation\\_keybert.py:104\u001b[39m, in \u001b[36mKeyBERTInspired.extract_topics\u001b[39m\u001b[34m(self, topic_model, documents, c_tf_idf, topics, embeddings)\u001b[39m\n\u001b[32m    100\u001b[39m topics = \u001b[38;5;28mself\u001b[39m._extract_candidate_words(topic_model, c_tf_idf, topics)\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# We calculate the similarity between word and document embeddings and create\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# topic embeddings from the representative document embeddings\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m sim_matrix, words = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentative_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_doc_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_embeddings\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Find the best matching words based on the similarity matrix for each topic\u001b[39;00m\n\u001b[32m    108\u001b[39m updated_topics = \u001b[38;5;28mself\u001b[39m._extract_top_words(words, topics, sim_matrix)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\representation\\_keybert.py:189\u001b[39m, in \u001b[36mKeyBERTInspired._extract_embeddings\u001b[39m\u001b[34m(self, topic_model, topics, representative_docs, repr_doc_indices, repr_embeddings)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# Calculate word embeddings and extract best matching with updated topic_embeddings\u001b[39;00m\n\u001b[32m    188\u001b[39m vocab = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m topics.values() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]))\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m word_embeddings = \u001b[43mtopic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_extract_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocument\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m sim = cosine_similarity(topic_embeddings, word_embeddings)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sim, vocab\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:3729\u001b[39m, in \u001b[36mBERTopic._extract_embeddings\u001b[39m\u001b[34m(self, documents, images, method, verbose)\u001b[39m\n\u001b[32m   3727\u001b[39m     embeddings = \u001b[38;5;28mself\u001b[39m.embedding_model.embed_words(words=documents, verbose=verbose)\n\u001b[32m   3728\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mdocument\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3729\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3730\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m documents[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m images \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3731\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3732\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMake sure to use an embedding model that can either embed documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3733\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mor images depending on which you want to embed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3734\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\backend\\_base.py:62\u001b[39m, in \u001b[36mBaseEmbedder.embed_documents\u001b[39m\u001b[34m(self, document, verbose)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, document: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> np.ndarray:\n\u001b[32m     51\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed a list of n words into an n-dimensional\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    matrix of embeddings.\u001b[39;00m\n\u001b[32m     53\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     60\u001b[39m \u001b[33;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\backend\\_sentencetransformers.py:84\u001b[39m, in \u001b[36mSentenceTransformerBackend.embed\u001b[39m\u001b[34m(self, documents, verbose)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed\u001b[39m(\u001b[38;5;28mself\u001b[39m, documents: List[\u001b[38;5;28mstr\u001b[39m], verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> np.ndarray:\n\u001b[32m     73\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[33;03m    matrix of embeddings.\u001b[39;00m\n\u001b[32m     75\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33;03m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:486\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    485\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    495\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:331\u001b[39m, in \u001b[36mMPNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    322\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    323\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    329\u001b[39m     **kwargs,\n\u001b[32m    330\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     position_bias = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_position_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     all_hidden_states = () \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    333\u001b[39m     all_attentions = () \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:374\u001b[39m, in \u001b[36mMPNetEncoder.compute_position_bias\u001b[39m\u001b[34m(self, x, position_ids, num_buckets)\u001b[39m\n\u001b[32m    370\u001b[39m     memory_position = torch.arange(klen, dtype=torch.long)[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[32m    372\u001b[39m relative_position = memory_position - context_position\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m rp_bucket = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrelative_position_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelative_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_buckets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_buckets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    375\u001b[39m rp_bucket = rp_bucket.to(x.device)\n\u001b[32m    376\u001b[39m values = \u001b[38;5;28mself\u001b[39m.relative_attention_bias(rp_bucket)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:394\u001b[39m, in \u001b[36mMPNetEncoder.relative_position_bucket\u001b[39m\u001b[34m(relative_position, num_buckets, max_distance)\u001b[39m\n\u001b[32m    390\u001b[39m max_exact = num_buckets // \u001b[32m2\u001b[39m\n\u001b[32m    391\u001b[39m is_small = n < max_exact\n\u001b[32m    393\u001b[39m val_if_large = max_exact + (\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_exact\u001b[49m\u001b[43m)\u001b[49m / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n\u001b[32m    395\u001b[39m ).to(torch.long)\n\u001b[32m    397\u001b[39m val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - \u001b[32m1\u001b[39m))\n\u001b[32m    398\u001b[39m ret += torch.where(is_small, n, val_if_large)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for embedding_model_name in embedding_model_names:\n",
    "    embedding_model = SentenceTransformer(embedding_model_name, device=DEVICE)\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "        for n_components in n_components_range:\n",
    "            for min_cluster_size in min_cluster_size_range:\n",
    "                model_name = f\"{embedding_model_name}_{n_neighbors}_{n_components}_{min_cluster_size}\"\n",
    "                if model_name in trained_models[\"model_name\"].values:\n",
    "                    print(f\"{model_name} had already been trained\")\n",
    "                    continue\n",
    "                else:\n",
    "                    start_time = time.time()\n",
    "                    model_name = train_bertopic(embedding_model_name,n_neighbors,n_components,min_cluster_size,embedding_model)\n",
    "                    end_time = time.time()\n",
    "                    train_time = end_time-start_time\n",
    "                    \n",
    "                    # Write to training log\n",
    "                    new_row = pd.DataFrame({\"model_name\": [model_name],\"train_time\":[train_time]})\n",
    "                    trained_models = pd.concat([trained_models, new_row], ignore_index=True)\n",
    "                    trained_models.to_csv(MODEL_TRAINING_LOG, index=False)\n",
    "                    \n",
    "                    # Print Status\n",
    "                    print(f\"Trained {model_name} in {train_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26121545-a4fc-4294-92b2-ea8f39a1183c",
   "metadata": {},
   "source": [
    "## 5. Evaluation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8514e4-746f-42ab-a6b2-eacbee47dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get test_data\n",
    "os.makedirs(TEST_DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "tokenized_texts_file = os.path.join(TEST_DATA_FOLDER, \"tokenized_texts.pkl\")\n",
    "dictionary_file = os.path.join(TEST_DATA_FOLDER, \"dictionary.pkl\")\n",
    "texts_clean_file = os.path.join(TEST_DATA_FOLDER, \"texts_clean.pkl\")\n",
    "\n",
    "tokenized_texts = None\n",
    "dictionary = None\n",
    "\n",
    "if os.path.exists(tokenized_texts_file) and os.path.exists(dictionary_file):\n",
    "    with open(tokenized_texts_file, \"rb\") as f:\n",
    "        tokenized_texts = pickle.load(f)\n",
    "    with open(dictionary_file, \"rb\") as f:\n",
    "        dictionary = pickle.load(f)\n",
    "    with open(texts_clean_file, \"rb\") as f:\n",
    "        texts_clean = pickle.load(f)\n",
    "else:\n",
    "    # Tokenize Document\n",
    "    tokenized_texts = [[str(token) for token in doc.split() if token.strip() != ''] for doc in texts_clean]\n",
    "    # Create Dictionary\n",
    "    dictionary = Dictionary(tokenized_texts)\n",
    "    \n",
    "    with open(tokenized_texts_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_texts, f)\n",
    "    with open(dictionary_file, \"wb\") as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "    with open(texts_clean_file, \"wb\") as f:\n",
    "        pickle.dump(texts_clean, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c449dd1-efcc-4884-8a8f-2afd2fd58ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monogram_coherence_score(model,embedding_model,tokenized_texts,dictionary):\n",
    "    monogram_topic_model = BERTopic.load(model,embedding_model=embedding_model)\n",
    "    monogram_topic_model.get_topic_info()\n",
    "    \n",
    "    # Extract Topics\n",
    "    # Filter topic words to exist in the dictionary\n",
    "    topics = [\n",
    "        [str(word) for word, _ in words_probs if str(word) in dictionary.token2id]\n",
    "        for topic_id, words_probs in monogram_topic_model.get_topics().items()\n",
    "        if topic_id != -1\n",
    "    ]\n",
    "    \n",
    "    # Remove empty topics (just in case)\n",
    "    topics = [t for t in topics if len(t) > 0]\n",
    "    \n",
    "    # Compute Coherence\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topics,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    monogram_coherence = coherence_model.get_coherence()\n",
    "    \n",
    "    return monogram_coherence\n",
    "\n",
    "def multigram_coherence_score(model,embedding_model,tokenized_texts,dictionary,texts_clean):\n",
    "    multigram_topic_model = BERTopic.load(model,embedding_model=embedding_model)\n",
    "    multigram_topic_model.update_topics(texts_clean, vectorizer_model=CountVectorizer(stop_words=\"english\", ngram_range=(2,3)))\n",
    "    multigram_topic_model.get_topic_info()\n",
    "    \n",
    "    # Topics have to be split into singular words\n",
    "    topics = [\n",
    "        sum([word.split() for word, _ in multigram_topic_model.get_topic(topic)], [])\n",
    "        for topic in multigram_topic_model.get_topics().keys()\n",
    "        if topic != -1\n",
    "    ]\n",
    "    \n",
    "    # Remove empty topics (just in case)\n",
    "    topics = [t for t in topics if len(t) > 0]\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topics,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    multigram_coherence = coherence_model.get_coherence()\n",
    "    return multigram_coherence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071edf90-3c27-48d8-994e-3dbf35c2803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise logs \n",
    "training_log_df = pd.read_csv(\"training_log.csv\")\n",
    "evaluation_log_df = None\n",
    "\n",
    "# load evaluation into df\n",
    "if os.path.exists(MODEL_EVALUATION_LOG):\n",
    "    evaluation_log_df = pd.read_csv(MODEL_EVALUATION_LOG)\n",
    "else:\n",
    "    evaluation_log_df = pd.DataFrame(columns=[\"model_name\", \"train_time\", \"monogram_cv\", \"multigram_cv\", \"eval_time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41099078-b4bc-45bc-8fa5-24db41aebaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, row in training_log_df.iterrows():\n",
    "    model_name = row[\"model_name\"]\n",
    "    train_time = row[\"train_time\"]\n",
    "\n",
    "    # Skip evaluation if evaluated before\n",
    "    if model_name in evaluation_log_df[\"model_name\"].values:\n",
    "        print(f\"{model_name} has already evaluated.\")\n",
    "        continue\n",
    "    \n",
    "    model = os.path.join(MODEL_FOLDER,model_name)\n",
    "    embedding_model = SentenceTransformer(\"all-mpnet-base-v2\",device=DEVICE) # Hardcoded for quick test, will change later\n",
    "\n",
    "    start_time = time.time()\n",
    "    monogram_cv = monogram_coherence_score(model,embedding_model,tokenized_texts,dictionary)\n",
    "    multigram_cv = multigram_coherence_score(model,embedding_model,tokenized_texts,dictionary,texts_clean)\n",
    "    end_time = time.time()\n",
    "    eval_time = end_time-start_time\n",
    "\n",
    "    print(f\"{model_name} in {eval_time} : {monogram_cv}, {multigram_cv}\")\n",
    "\n",
    "    new_row = {\n",
    "        \"model_name\": model_name,\n",
    "        \"train_time\": train_time,\n",
    "        \"monogram_cv\": monogram_cv,\n",
    "        \"multigram_cv\": multigram_cv,\n",
    "        \"eval_time\": eval_time\n",
    "    }\n",
    "\n",
    "    evaluation_log_df = pd.concat([evaluation_log_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    evaluation_log_df.to_csv(MODEL_EVALUATION_LOG, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4125b8-9e2d-4bd9-a3d8-c3d1ca8d7c06",
   "metadata": {},
   "source": [
    "## 6. Result (Best Model) - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ee959-9d1c-423b-9c3d-5ce4a70de563",
   "metadata": {},
   "source": [
    "### Topic Info (Monogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e19e6-7e0c-4104-adb3-d79e6b736c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "monogram_topic_model = BERTopic.load(\"topic_model\",embedding_model=embedding_model)\n",
    "monogram_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc36926-f73b-4e6d-94f2-cb9f434236e1",
   "metadata": {},
   "source": [
    "### Topic Info (Multigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dcec9-f8f7-492d-9535-37e9c202853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multigram_topic_model = BERTopic.load(\"topic_model\",embedding_model=embedding_model)\n",
    "multigram_topic_model.update_topics(texts_clean, vectorizer_model=CountVectorizer(stop_words=\"english\", ngram_range=(2,3)))\n",
    "multigram_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3afba-bedb-4e22-a531-148e71e23202",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a63bc3-db86-4f32-8ee4-2de13c4c4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Tokenize Document\n",
    "tokenized_texts = [[str(token) for token in doc.split() if token.strip() != ''] for doc in texts_clean]\n",
    "\n",
    "# Create Dictionary\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Extract Topics\n",
    "# Filter topic words to exist in the dictionary\n",
    "topics = [\n",
    "    [str(word) for word, _ in words_probs if str(word) in dictionary.token2id]\n",
    "    for topic_id, words_probs in monogram_topic_model.get_topics().items()\n",
    "    if topic_id != -1\n",
    "]\n",
    "\n",
    "# Remove empty topics (just in case)\n",
    "topics = [t for t in topics if len(t) > 0]\n",
    "\n",
    "# Compute Coherence\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "monogram_coherence = coherence_model.get_coherence()\n",
    "print(\"Monogram C_v Coherence:\", monogram_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e1f9f-b717-42ef-8bf7-c6835d9beebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [doc.split() for doc in texts_clean]\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Topics have to be split into singular words\n",
    "topics = [\n",
    "    sum([word.split() for word, _ in multigram_topic_model.get_topic(topic)], [])\n",
    "    for topic in multigram_topic_model.get_topics().keys()\n",
    "    if topic != -1\n",
    "]\n",
    "\n",
    "# Remove empty topics (just in case)\n",
    "topics = [t for t in topics if len(t) > 0]\n",
    "\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "multigram_coherence = coherence_model.get_coherence()\n",
    "print(\"Multigram C_v Coherence:\", multigram_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8d8dd-4466-42b9-bc84-b0d62d2bec92",
   "metadata": {},
   "source": [
    "## 7. Using LLM to Improve Representation (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ab163-622c-44e3-94df-530c09acc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from bertopic.representation import OpenAI\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "topic_model.update_topics(texts_clean, representation_model=OpenAI(client, model=\"gpt-4o-mini\", delay_in_seconds=3))\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0344cd0-ca4e-41cd-825d-c31ac30c24c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
