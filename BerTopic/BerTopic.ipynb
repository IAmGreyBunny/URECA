{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a0141ee-5d71-4b76-b6ed-e9dd004ee78f",
   "metadata": {},
   "source": [
    "# BerTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e2519b-9b78-4f2f-8b44-4e122454a76d",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f390a36-418b-46d1-8bfa-4222f6d20a88",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b5f133-e723-4eb5-bfb9-65e6e09b76ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158ea9b5-dd74-4db6-a1ee-4979a3ad55e1",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4906a23-e197-4567-9212-87e044e93a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FOLDER = \"models\"\n",
    "BEST_MODEL_NAME = \"best_model\"\n",
    "DATASET_FOLDER = \"data\"\n",
    "TEST_DATA_FOLDER = \"test_data\"\n",
    "MODEL_TRAINING_LOG = \"training_log.csv\"\n",
    "MODEL_EVALUATION_LOG = \"eval_log.csv\"\n",
    "RESULT_FILE = \"result.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814009a-7594-4d5f-ae04-e4716652cd4d",
   "metadata": {},
   "source": [
    "### Common Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45b26b6-242e-4a7c-a2d4-5a8567672ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ea2ac-f5bb-4932-9c5f-aad0eddf9483",
   "metadata": {},
   "source": [
    "### Packages for Bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bdf2814-5f9c-40b5-a1dd-246d96e4a6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\URECA\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d6b4d-cad2-4a95-ab8f-d65854895112",
   "metadata": {},
   "source": [
    "### Packages for Gensim Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8b2bca5-9e7e-4fe7-8095-5a75ef4ac4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779d8aaf-3451-408d-95ce-73b74f309146",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ec847-26c1-452f-83b7-e8e7e26373e2",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f39fe2-df09-4574-b96b-3faec65295d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0269c8-0ae0-4e07-8469-f6874552532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"SetFit/20_newsgroups\")\n",
    "random.seed(42)\n",
    "text_label = list(zip(dataset[\"train\"][\"text\"], dataset[\"train\"][\"label_text\"]))\n",
    "sampled_text_label = random.sample(text_label, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b4473-3672-4732-9298-0ab83a5f262a",
   "metadata": {},
   "source": [
    "### Clean Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12bc7720-e7f8-4749-837a-fc8eded30b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_for_embedding(text, max_sentences=5):\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.strip().startswith(\">\")]\n",
    "    lines = [line for line in lines if not re.match(r\"^\\s*(from|subject|organization|lines|writes|article)\\s*:\", line, re.IGNORECASE)]\n",
    "    text = \" \".join(lines)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r\"[!?]{3,}\", \"\", text)\n",
    "    sentence_split = re.split(r'(?<=[.!?]) +', text)\n",
    "    sentence_split = [\n",
    "        s for s in sentence_split\n",
    "        if len(s.strip()) > 15 and not s.strip().isupper()\n",
    "      ]\n",
    "    return \" \".join(sentence_split[:max_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d4dfa8-e6f8-4b7d-bac8-5275fc4ba388",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_clean = [clean_for_embedding(text) for text,_ in sampled_text_label]\n",
    "labels = [label for _, label in sampled_text_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075daa7a-337d-45bc-9a5b-5f6247de0ba7",
   "metadata": {},
   "source": [
    "## 3. BerTopic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e53c0e39-9fd2-4331-8962-9bf79a481fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bertopic(embedding_model_name,n_neighbors,n_components,min_cluster_size,embedding_model=None):\n",
    "    # Step 1 - Extract embeddings\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "    if embedding_model is None:\n",
    "        embedding_model = SentenceTransformer(embedding_model_name, device=DEVICE)\n",
    "\n",
    "    model_name = f\"{embedding_model_name}_{n_neighbors}_{n_components}_{min_cluster_size}\"\n",
    "    \n",
    "    # Step 2 - Reduce dimensionality\n",
    "    umap_model = UMAP(n_neighbors=n_neighbors, n_components=n_components, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    \n",
    "    # Step 3 - Cluster reduced embeddings\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster_size, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "    \n",
    "    # Step 4 - Tokenize topics\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    \n",
    "    # Step 5 - Create topic representation\n",
    "    ctfidf_model = ClassTfidfTransformer()\n",
    "    \n",
    "    # Step 6 - (Optional) Fine-tune topic representations with\n",
    "    # a `bertopic.representation` model\n",
    "    representation_model = KeyBERTInspired()\n",
    "    \n",
    "    # All steps together\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model, # Step 1 - Extract embeddings\n",
    "        umap_model=umap_model, # Step 2 - Reduce dimensionality\n",
    "        hdbscan_model=hdbscan_model, # Step 3 - Cluster reduced embeddings\n",
    "        vectorizer_model=vectorizer_model, # Step 4 - Tokenize topics\n",
    "        ctfidf_model=ctfidf_model, # Step 5 - Extract topic words\n",
    "        representation_model=representation_model # Step 6 - (Optional) Fine-tune topic representations\n",
    "    )\n",
    "    topics, probs = topic_model.fit_transform(texts_clean)\n",
    "    \n",
    "    topic_model.save(f\"{MODEL_FOLDER}/{model_name}\", serialization=\"pytorch\")\n",
    "\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2fc535-babf-4f49-8179-4024fd150e2f",
   "metadata": {},
   "source": [
    "## 4. Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03083b60-6f21-4c58-890a-b3294aeeeb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models = pd.DataFrame(columns=[\"model_name\",\"train_time\"])\n",
    "\n",
    "# Create Log file if it doesn't exist\n",
    "if not os.path.exists(MODEL_TRAINING_LOG):\n",
    "    trained_models = pd.DataFrame(columns=[\"model_name\",\"train_time\"])\n",
    "    trained_models.to_csv(MODEL_TRAINING_LOG, index=False)\n",
    "else:\n",
    "    trained_models = pd.read_csv(MODEL_TRAINING_LOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb93757-cc6e-438a-8174-7b94441b3ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to test for\n",
    "embedding_model_names = [\"all-mpnet-base-v2\",\"all-MiniLM-L6-v2\"]\n",
    "n_neighbors_range = [5, 10, 15, 25, 50]\n",
    "n_components_range = [5, 10, 15, 20, 25]\n",
    "min_cluster_size_range = [5, 10, 15, 20, 30, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f92a7c2-4e2b-48e9-8ac6-53d8f7d22d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-mpnet-base-v2_5_5_5 had already been trained\n",
      "all-mpnet-base-v2_5_5_10 had already been trained\n",
      "all-mpnet-base-v2_5_5_15 had already been trained\n",
      "all-mpnet-base-v2_5_5_20 had already been trained\n",
      "all-mpnet-base-v2_5_5_30 had already been trained\n",
      "all-mpnet-base-v2_5_5_50 had already been trained\n",
      "all-mpnet-base-v2_5_10_5 had already been trained\n",
      "all-mpnet-base-v2_5_10_10 had already been trained\n",
      "all-mpnet-base-v2_5_10_15 had already been trained\n",
      "all-mpnet-base-v2_5_10_20 had already been trained\n",
      "all-mpnet-base-v2_5_10_30 had already been trained\n",
      "all-mpnet-base-v2_5_10_50 had already been trained\n",
      "all-mpnet-base-v2_5_15_5 had already been trained\n",
      "all-mpnet-base-v2_5_15_10 had already been trained\n",
      "all-mpnet-base-v2_5_15_15 had already been trained\n",
      "all-mpnet-base-v2_5_15_20 had already been trained\n",
      "all-mpnet-base-v2_5_15_30 had already been trained\n",
      "all-mpnet-base-v2_5_15_50 had already been trained\n",
      "all-mpnet-base-v2_5_20_5 had already been trained\n",
      "all-mpnet-base-v2_5_20_10 had already been trained\n",
      "all-mpnet-base-v2_5_20_15 had already been trained\n",
      "all-mpnet-base-v2_5_20_20 had already been trained\n",
      "all-mpnet-base-v2_5_20_30 had already been trained\n",
      "all-mpnet-base-v2_5_20_50 had already been trained\n",
      "all-mpnet-base-v2_5_25_5 had already been trained\n",
      "all-mpnet-base-v2_5_25_10 had already been trained\n",
      "all-mpnet-base-v2_5_25_15 had already been trained\n",
      "all-mpnet-base-v2_5_25_20 had already been trained\n",
      "all-mpnet-base-v2_5_25_30 had already been trained\n",
      "all-mpnet-base-v2_5_25_50 had already been trained\n",
      "all-mpnet-base-v2_10_5_5 had already been trained\n",
      "all-mpnet-base-v2_10_5_10 had already been trained\n",
      "all-mpnet-base-v2_10_5_15 had already been trained\n",
      "all-mpnet-base-v2_10_5_20 had already been trained\n",
      "all-mpnet-base-v2_10_5_30 had already been trained\n",
      "all-mpnet-base-v2_10_5_50 had already been trained\n",
      "all-mpnet-base-v2_10_10_5 had already been trained\n",
      "all-mpnet-base-v2_10_10_10 had already been trained\n",
      "all-mpnet-base-v2_10_10_15 had already been trained\n",
      "all-mpnet-base-v2_10_10_20 had already been trained\n",
      "all-mpnet-base-v2_10_10_30 had already been trained\n",
      "all-mpnet-base-v2_10_10_50 had already been trained\n",
      "all-mpnet-base-v2_10_15_5 had already been trained\n",
      "all-mpnet-base-v2_10_15_10 had already been trained\n",
      "all-mpnet-base-v2_10_15_15 had already been trained\n",
      "all-mpnet-base-v2_10_15_20 had already been trained\n",
      "all-mpnet-base-v2_10_15_30 had already been trained\n",
      "all-mpnet-base-v2_10_15_50 had already been trained\n",
      "all-mpnet-base-v2_10_20_5 had already been trained\n",
      "all-mpnet-base-v2_10_20_10 had already been trained\n",
      "all-mpnet-base-v2_10_20_15 had already been trained\n",
      "all-mpnet-base-v2_10_20_20 had already been trained\n",
      "all-mpnet-base-v2_10_20_30 had already been trained\n",
      "all-mpnet-base-v2_10_20_50 had already been trained\n",
      "all-mpnet-base-v2_10_25_5 had already been trained\n",
      "all-mpnet-base-v2_10_25_10 had already been trained\n",
      "all-mpnet-base-v2_10_25_15 had already been trained\n",
      "all-mpnet-base-v2_10_25_20 had already been trained\n",
      "all-mpnet-base-v2_10_25_30 had already been trained\n",
      "all-mpnet-base-v2_10_25_50 had already been trained\n",
      "all-mpnet-base-v2_15_5_5 had already been trained\n",
      "all-mpnet-base-v2_15_5_10 had already been trained\n",
      "all-mpnet-base-v2_15_5_15 had already been trained\n",
      "all-mpnet-base-v2_15_5_20 had already been trained\n",
      "all-mpnet-base-v2_15_5_30 had already been trained\n",
      "all-mpnet-base-v2_15_5_50 had already been trained\n",
      "all-mpnet-base-v2_15_10_5 had already been trained\n",
      "all-mpnet-base-v2_15_10_10 had already been trained\n",
      "all-mpnet-base-v2_15_10_15 had already been trained\n",
      "all-mpnet-base-v2_15_10_20 had already been trained\n",
      "all-mpnet-base-v2_15_10_30 had already been trained\n",
      "all-mpnet-base-v2_15_10_50 had already been trained\n",
      "all-mpnet-base-v2_15_15_5 had already been trained\n",
      "all-mpnet-base-v2_15_15_10 had already been trained\n",
      "all-mpnet-base-v2_15_15_15 had already been trained\n",
      "all-mpnet-base-v2_15_15_20 had already been trained\n",
      "all-mpnet-base-v2_15_15_30 had already been trained\n",
      "all-mpnet-base-v2_15_15_50 had already been trained\n",
      "all-mpnet-base-v2_15_20_5 had already been trained\n",
      "all-mpnet-base-v2_15_20_10 had already been trained\n",
      "all-mpnet-base-v2_15_20_15 had already been trained\n",
      "all-mpnet-base-v2_15_20_20 had already been trained\n",
      "all-mpnet-base-v2_15_20_30 had already been trained\n",
      "all-mpnet-base-v2_15_20_50 had already been trained\n",
      "all-mpnet-base-v2_15_25_5 had already been trained\n",
      "all-mpnet-base-v2_15_25_10 had already been trained\n",
      "all-mpnet-base-v2_15_25_15 had already been trained\n",
      "all-mpnet-base-v2_15_25_20 had already been trained\n",
      "all-mpnet-base-v2_15_25_30 had already been trained\n",
      "all-mpnet-base-v2_15_25_50 had already been trained\n",
      "all-mpnet-base-v2_25_5_5 had already been trained\n",
      "all-mpnet-base-v2_25_5_10 had already been trained\n",
      "all-mpnet-base-v2_25_5_15 had already been trained\n",
      "all-mpnet-base-v2_25_5_20 had already been trained\n",
      "all-mpnet-base-v2_25_5_30 had already been trained\n",
      "all-mpnet-base-v2_25_5_50 had already been trained\n",
      "all-mpnet-base-v2_25_10_5 had already been trained\n",
      "all-mpnet-base-v2_25_10_10 had already been trained\n",
      "all-mpnet-base-v2_25_10_15 had already been trained\n",
      "all-mpnet-base-v2_25_10_20 had already been trained\n",
      "all-mpnet-base-v2_25_10_30 had already been trained\n",
      "all-mpnet-base-v2_25_10_50 had already been trained\n",
      "all-mpnet-base-v2_25_15_5 had already been trained\n",
      "all-mpnet-base-v2_25_15_10 had already been trained\n",
      "all-mpnet-base-v2_25_15_15 had already been trained\n",
      "all-mpnet-base-v2_25_15_20 had already been trained\n",
      "all-mpnet-base-v2_25_15_30 had already been trained\n",
      "all-mpnet-base-v2_25_15_50 had already been trained\n",
      "all-mpnet-base-v2_25_20_5 had already been trained\n",
      "all-mpnet-base-v2_25_20_10 had already been trained\n",
      "all-mpnet-base-v2_25_20_15 had already been trained\n",
      "all-mpnet-base-v2_25_20_20 had already been trained\n",
      "all-mpnet-base-v2_25_20_30 had already been trained\n",
      "all-mpnet-base-v2_25_20_50 had already been trained\n",
      "all-mpnet-base-v2_25_25_5 had already been trained\n",
      "all-mpnet-base-v2_25_25_10 had already been trained\n",
      "all-mpnet-base-v2_25_25_15 had already been trained\n",
      "all-mpnet-base-v2_25_25_20 had already been trained\n",
      "all-mpnet-base-v2_25_25_30 had already been trained\n",
      "all-mpnet-base-v2_25_25_50 had already been trained\n",
      "all-mpnet-base-v2_50_5_5 had already been trained\n",
      "all-mpnet-base-v2_50_5_10 had already been trained\n",
      "all-mpnet-base-v2_50_5_15 had already been trained\n",
      "all-mpnet-base-v2_50_5_20 had already been trained\n",
      "all-mpnet-base-v2_50_5_30 had already been trained\n",
      "all-mpnet-base-v2_50_5_50 had already been trained\n",
      "all-mpnet-base-v2_50_10_5 had already been trained\n",
      "all-mpnet-base-v2_50_10_10 had already been trained\n",
      "all-mpnet-base-v2_50_10_15 had already been trained\n",
      "all-mpnet-base-v2_50_10_20 had already been trained\n",
      "all-mpnet-base-v2_50_10_30 had already been trained\n",
      "all-mpnet-base-v2_50_10_50 had already been trained\n",
      "all-mpnet-base-v2_50_15_5 had already been trained\n",
      "all-mpnet-base-v2_50_15_10 had already been trained\n",
      "all-mpnet-base-v2_50_15_15 had already been trained\n",
      "all-mpnet-base-v2_50_15_20 had already been trained\n",
      "all-mpnet-base-v2_50_15_30 had already been trained\n",
      "all-mpnet-base-v2_50_15_50 had already been trained\n",
      "all-mpnet-base-v2_50_20_5 had already been trained\n",
      "all-mpnet-base-v2_50_20_10 had already been trained\n",
      "all-mpnet-base-v2_50_20_15 had already been trained\n",
      "all-mpnet-base-v2_50_20_20 had already been trained\n",
      "all-mpnet-base-v2_50_20_30 had already been trained\n",
      "all-mpnet-base-v2_50_20_50 had already been trained\n",
      "all-mpnet-base-v2_50_25_5 had already been trained\n",
      "all-mpnet-base-v2_50_25_10 had already been trained\n",
      "all-mpnet-base-v2_50_25_15 had already been trained\n",
      "all-mpnet-base-v2_50_25_20 had already been trained\n",
      "all-mpnet-base-v2_50_25_30 had already been trained\n",
      "all-mpnet-base-v2_50_25_50 had already been trained\n",
      "all-MiniLM-L6-v2_5_5_5 had already been trained\n",
      "all-MiniLM-L6-v2_5_5_10 had already been trained\n",
      "all-MiniLM-L6-v2_5_5_15 had already been trained\n",
      "all-MiniLM-L6-v2_5_5_20 had already been trained\n",
      "all-MiniLM-L6-v2_5_5_30 had already been trained\n",
      "all-MiniLM-L6-v2_5_5_50 had already been trained\n",
      "all-MiniLM-L6-v2_5_10_5 had already been trained\n",
      "all-MiniLM-L6-v2_5_10_10 had already been trained\n",
      "all-MiniLM-L6-v2_5_10_15 had already been trained\n",
      "all-MiniLM-L6-v2_5_10_20 had already been trained\n",
      "all-MiniLM-L6-v2_5_10_30 had already been trained\n",
      "all-MiniLM-L6-v2_5_10_50 had already been trained\n",
      "all-MiniLM-L6-v2_5_15_5 had already been trained\n",
      "all-MiniLM-L6-v2_5_15_10 had already been trained\n",
      "all-MiniLM-L6-v2_5_15_15 had already been trained\n",
      "all-MiniLM-L6-v2_5_15_20 had already been trained\n",
      "all-MiniLM-L6-v2_5_15_30 had already been trained\n",
      "all-MiniLM-L6-v2_5_15_50 had already been trained\n",
      "all-MiniLM-L6-v2_5_20_5 had already been trained\n",
      "all-MiniLM-L6-v2_5_20_10 had already been trained\n",
      "all-MiniLM-L6-v2_5_20_15 had already been trained\n",
      "all-MiniLM-L6-v2_5_20_20 had already been trained\n",
      "all-MiniLM-L6-v2_5_20_30 had already been trained\n",
      "all-MiniLM-L6-v2_5_20_50 had already been trained\n",
      "all-MiniLM-L6-v2_5_25_5 had already been trained\n",
      "all-MiniLM-L6-v2_5_25_10 had already been trained\n",
      "all-MiniLM-L6-v2_5_25_15 had already been trained\n",
      "all-MiniLM-L6-v2_5_25_20 had already been trained\n",
      "all-MiniLM-L6-v2_5_25_30 had already been trained\n",
      "all-MiniLM-L6-v2_5_25_50 had already been trained\n",
      "all-MiniLM-L6-v2_10_5_5 had already been trained\n",
      "all-MiniLM-L6-v2_10_5_10 had already been trained\n",
      "all-MiniLM-L6-v2_10_5_15 had already been trained\n",
      "all-MiniLM-L6-v2_10_5_20 had already been trained\n",
      "all-MiniLM-L6-v2_10_5_30 had already been trained\n",
      "all-MiniLM-L6-v2_10_5_50 had already been trained\n",
      "all-MiniLM-L6-v2_10_10_5 had already been trained\n",
      "all-MiniLM-L6-v2_10_10_10 had already been trained\n",
      "all-MiniLM-L6-v2_10_10_15 had already been trained\n",
      "all-MiniLM-L6-v2_10_10_20 had already been trained\n",
      "all-MiniLM-L6-v2_10_10_30 had already been trained\n",
      "all-MiniLM-L6-v2_10_10_50 had already been trained\n",
      "all-MiniLM-L6-v2_10_15_5 had already been trained\n",
      "all-MiniLM-L6-v2_10_15_10 had already been trained\n",
      "all-MiniLM-L6-v2_10_15_15 had already been trained\n",
      "all-MiniLM-L6-v2_10_15_20 had already been trained\n",
      "all-MiniLM-L6-v2_10_15_30 had already been trained\n",
      "all-MiniLM-L6-v2_10_15_50 had already been trained\n",
      "all-MiniLM-L6-v2_10_20_5 had already been trained\n",
      "all-MiniLM-L6-v2_10_20_10 had already been trained\n",
      "all-MiniLM-L6-v2_10_20_15 had already been trained\n",
      "all-MiniLM-L6-v2_10_20_20 had already been trained\n",
      "all-MiniLM-L6-v2_10_20_30 had already been trained\n",
      "all-MiniLM-L6-v2_10_20_50 had already been trained\n",
      "all-MiniLM-L6-v2_10_25_5 had already been trained\n",
      "all-MiniLM-L6-v2_10_25_10 had already been trained\n",
      "all-MiniLM-L6-v2_10_25_15 had already been trained\n",
      "all-MiniLM-L6-v2_10_25_20 had already been trained\n",
      "all-MiniLM-L6-v2_10_25_30 had already been trained\n",
      "all-MiniLM-L6-v2_10_25_50 had already been trained\n",
      "all-MiniLM-L6-v2_15_5_5 had already been trained\n",
      "all-MiniLM-L6-v2_15_5_10 had already been trained\n",
      "all-MiniLM-L6-v2_15_5_15 had already been trained\n",
      "all-MiniLM-L6-v2_15_5_20 had already been trained\n",
      "all-MiniLM-L6-v2_15_5_30 had already been trained\n",
      "all-MiniLM-L6-v2_15_5_50 had already been trained\n",
      "all-MiniLM-L6-v2_15_10_5 had already been trained\n",
      "all-MiniLM-L6-v2_15_10_10 had already been trained\n",
      "all-MiniLM-L6-v2_15_10_15 had already been trained\n",
      "all-MiniLM-L6-v2_15_10_20 had already been trained\n",
      "all-MiniLM-L6-v2_15_10_30 had already been trained\n",
      "all-MiniLM-L6-v2_15_10_50 had already been trained\n",
      "all-MiniLM-L6-v2_15_15_5 had already been trained\n",
      "all-MiniLM-L6-v2_15_15_10 had already been trained\n",
      "all-MiniLM-L6-v2_15_15_15 had already been trained\n",
      "all-MiniLM-L6-v2_15_15_20 had already been trained\n",
      "all-MiniLM-L6-v2_15_15_30 had already been trained\n",
      "all-MiniLM-L6-v2_15_15_50 had already been trained\n",
      "all-MiniLM-L6-v2_15_20_5 had already been trained\n",
      "all-MiniLM-L6-v2_15_20_10 had already been trained\n",
      "all-MiniLM-L6-v2_15_20_15 had already been trained\n",
      "all-MiniLM-L6-v2_15_20_20 had already been trained\n",
      "all-MiniLM-L6-v2_15_20_30 had already been trained\n",
      "all-MiniLM-L6-v2_15_20_50 had already been trained\n",
      "all-MiniLM-L6-v2_15_25_5 had already been trained\n",
      "all-MiniLM-L6-v2_15_25_10 had already been trained\n",
      "all-MiniLM-L6-v2_15_25_15 had already been trained\n",
      "all-MiniLM-L6-v2_15_25_20 had already been trained\n",
      "all-MiniLM-L6-v2_15_25_30 had already been trained\n",
      "all-MiniLM-L6-v2_15_25_50 had already been trained\n",
      "all-MiniLM-L6-v2_25_5_5 had already been trained\n",
      "all-MiniLM-L6-v2_25_5_10 had already been trained\n",
      "all-MiniLM-L6-v2_25_5_15 had already been trained\n",
      "all-MiniLM-L6-v2_25_5_20 had already been trained\n",
      "all-MiniLM-L6-v2_25_5_30 had already been trained\n",
      "all-MiniLM-L6-v2_25_5_50 had already been trained\n",
      "all-MiniLM-L6-v2_25_10_5 had already been trained\n",
      "all-MiniLM-L6-v2_25_10_10 had already been trained\n",
      "all-MiniLM-L6-v2_25_10_15 had already been trained\n",
      "all-MiniLM-L6-v2_25_10_20 had already been trained\n",
      "all-MiniLM-L6-v2_25_10_30 had already been trained\n",
      "all-MiniLM-L6-v2_25_10_50 had already been trained\n",
      "all-MiniLM-L6-v2_25_15_5 had already been trained\n",
      "all-MiniLM-L6-v2_25_15_10 had already been trained\n",
      "all-MiniLM-L6-v2_25_15_15 had already been trained\n",
      "all-MiniLM-L6-v2_25_15_20 had already been trained\n",
      "all-MiniLM-L6-v2_25_15_30 had already been trained\n",
      "all-MiniLM-L6-v2_25_15_50 had already been trained\n",
      "all-MiniLM-L6-v2_25_20_5 had already been trained\n",
      "all-MiniLM-L6-v2_25_20_10 had already been trained\n",
      "all-MiniLM-L6-v2_25_20_15 had already been trained\n",
      "all-MiniLM-L6-v2_25_20_20 had already been trained\n",
      "all-MiniLM-L6-v2_25_20_30 had already been trained\n",
      "all-MiniLM-L6-v2_25_20_50 had already been trained\n",
      "all-MiniLM-L6-v2_25_25_5 had already been trained\n",
      "all-MiniLM-L6-v2_25_25_10 had already been trained\n",
      "all-MiniLM-L6-v2_25_25_15 had already been trained\n",
      "all-MiniLM-L6-v2_25_25_20 had already been trained\n",
      "all-MiniLM-L6-v2_25_25_30 had already been trained\n",
      "all-MiniLM-L6-v2_25_25_50 had already been trained\n",
      "all-MiniLM-L6-v2_50_5_5 had already been trained\n",
      "all-MiniLM-L6-v2_50_5_10 had already been trained\n",
      "all-MiniLM-L6-v2_50_5_15 had already been trained\n",
      "all-MiniLM-L6-v2_50_5_20 had already been trained\n",
      "all-MiniLM-L6-v2_50_5_30 had already been trained\n",
      "all-MiniLM-L6-v2_50_5_50 had already been trained\n",
      "all-MiniLM-L6-v2_50_10_5 had already been trained\n",
      "all-MiniLM-L6-v2_50_10_10 had already been trained\n",
      "all-MiniLM-L6-v2_50_10_15 had already been trained\n",
      "all-MiniLM-L6-v2_50_10_20 had already been trained\n",
      "all-MiniLM-L6-v2_50_10_30 had already been trained\n",
      "all-MiniLM-L6-v2_50_10_50 had already been trained\n",
      "all-MiniLM-L6-v2_50_15_5 had already been trained\n",
      "all-MiniLM-L6-v2_50_15_10 had already been trained\n",
      "all-MiniLM-L6-v2_50_15_15 had already been trained\n",
      "all-MiniLM-L6-v2_50_15_20 had already been trained\n",
      "all-MiniLM-L6-v2_50_15_30 had already been trained\n",
      "all-MiniLM-L6-v2_50_15_50 had already been trained\n",
      "all-MiniLM-L6-v2_50_20_5 had already been trained\n",
      "all-MiniLM-L6-v2_50_20_10 had already been trained\n",
      "all-MiniLM-L6-v2_50_20_15 had already been trained\n",
      "all-MiniLM-L6-v2_50_20_20 had already been trained\n",
      "all-MiniLM-L6-v2_50_20_30 had already been trained\n",
      "all-MiniLM-L6-v2_50_20_50 had already been trained\n",
      "all-MiniLM-L6-v2_50_25_5 had already been trained\n",
      "all-MiniLM-L6-v2_50_25_10 had already been trained\n",
      "all-MiniLM-L6-v2_50_25_15 had already been trained\n",
      "all-MiniLM-L6-v2_50_25_20 had already been trained\n",
      "all-MiniLM-L6-v2_50_25_30 had already been trained\n",
      "all-MiniLM-L6-v2_50_25_50 had already been trained\n"
     ]
    }
   ],
   "source": [
    "for embedding_model_name in embedding_model_names:\n",
    "    embedding_model = SentenceTransformer(embedding_model_name, device=DEVICE)\n",
    "    for n_neighbors in n_neighbors_range:\n",
    "        for n_components in n_components_range:\n",
    "            for min_cluster_size in min_cluster_size_range:\n",
    "                model_name = f\"{embedding_model_name}_{n_neighbors}_{n_components}_{min_cluster_size}\"\n",
    "                if model_name in trained_models[\"model_name\"].values:\n",
    "                    print(f\"{model_name} had already been trained\")\n",
    "                    continue\n",
    "                else:\n",
    "                    start_time = time.time()\n",
    "                    model_name = train_bertopic(embedding_model_name,n_neighbors,n_components,min_cluster_size,embedding_model)\n",
    "                    end_time = time.time()\n",
    "                    train_time = end_time-start_time\n",
    "                    \n",
    "                    # Write to training log\n",
    "                    new_row = pd.DataFrame({\"model_name\": [model_name],\"train_time\":[train_time]})\n",
    "                    trained_models = pd.concat([trained_models, new_row], ignore_index=True)\n",
    "                    trained_models.to_csv(MODEL_TRAINING_LOG, index=False)\n",
    "                    \n",
    "                    # Print Status\n",
    "                    print(f\"Trained {model_name} in {train_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26121545-a4fc-4294-92b2-ea8f39a1183c",
   "metadata": {},
   "source": [
    "## 5. Evaluation of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c449dd1-efcc-4884-8a8f-2afd2fd58ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting coherence score\n",
    "\n",
    "def monogram_coherence_score(model,embedding_model,tokenized_texts,dictionary):\n",
    "    monogram_topic_model = BERTopic.load(model,embedding_model=embedding_model)\n",
    "    monogram_topic_model.get_topic_info()\n",
    "    \n",
    "    # Extract Topics\n",
    "    # Filter topic words to exist in the dictionary\n",
    "    topics = [\n",
    "        [str(word) for word, _ in words_probs if str(word) in dictionary.token2id]\n",
    "        for topic_id, words_probs in monogram_topic_model.get_topics().items()\n",
    "        if topic_id != -1\n",
    "    ]\n",
    "    \n",
    "    # Remove empty topics (just in case)\n",
    "    topics = [t for t in topics if len(t) > 0]\n",
    "    \n",
    "    # Compute Coherence\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topics,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    monogram_coherence = coherence_model.get_coherence()\n",
    "    \n",
    "    return monogram_coherence\n",
    "\n",
    "def multigram_coherence_score(model,embedding_model,tokenized_texts,dictionary,texts_clean):\n",
    "    multigram_topic_model = BERTopic.load(model,embedding_model=embedding_model)\n",
    "    multigram_topic_model.update_topics(texts_clean, vectorizer_model=CountVectorizer(stop_words=\"english\", ngram_range=(2,3)))\n",
    "    multigram_topic_model.get_topic_info()\n",
    "    \n",
    "    # Topics have to be split into singular words\n",
    "    topics = [\n",
    "        sum([word.split() for word, _ in multigram_topic_model.get_topic(topic)], [])\n",
    "        for topic in multigram_topic_model.get_topics().keys()\n",
    "        if topic != -1\n",
    "    ]\n",
    "    \n",
    "    # Remove empty topics (just in case)\n",
    "    topics = [t for t in topics if len(t) > 0]\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topics,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v'\n",
    "    )\n",
    "    \n",
    "    multigram_coherence = coherence_model.get_coherence()\n",
    "    return multigram_coherence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fb555ff-38cc-4fac-b14a-57be1b02d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for extracting hyperparameter from model name\n",
    "def parse_model_name(model_name):\n",
    "    pattern = r\"(.+)_(\\d+)_(\\d+)_(\\d+)$\"\n",
    "    match = re.match(pattern, model_name)\n",
    "    \n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid model name format: {model_name}\")\n",
    "    \n",
    "    embedding_model_name, n_neighbors, n_components, min_cluster_size = match.groups()\n",
    "    \n",
    "    return (\n",
    "        embedding_model_name,\n",
    "        int(n_neighbors),\n",
    "        int(n_components),\n",
    "        int(min_cluster_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c8514e4-746f-42ab-a6b2-eacbee47dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get test_data\n",
    "\n",
    "os.makedirs(TEST_DATA_FOLDER, exist_ok=True)\n",
    "\n",
    "tokenized_texts_file = os.path.join(TEST_DATA_FOLDER, \"tokenized_texts.pkl\")\n",
    "dictionary_file = os.path.join(TEST_DATA_FOLDER, \"dictionary.pkl\")\n",
    "texts_clean_file = os.path.join(TEST_DATA_FOLDER, \"texts_clean.pkl\")\n",
    "\n",
    "tokenized_texts = None\n",
    "dictionary = None\n",
    "\n",
    "if os.path.exists(tokenized_texts_file) and os.path.exists(dictionary_file):\n",
    "    with open(tokenized_texts_file, \"rb\") as f:\n",
    "        tokenized_texts = pickle.load(f)\n",
    "    with open(dictionary_file, \"rb\") as f:\n",
    "        dictionary = pickle.load(f)\n",
    "    with open(texts_clean_file, \"rb\") as f:\n",
    "        texts_clean = pickle.load(f)\n",
    "else:\n",
    "    # Tokenize Document\n",
    "    tokenized_texts = [[str(token) for token in doc.split() if token.strip() != ''] for doc in texts_clean]\n",
    "    # Create Dictionary\n",
    "    dictionary = Dictionary(tokenized_texts)\n",
    "    \n",
    "    with open(tokenized_texts_file, \"wb\") as f:\n",
    "        pickle.dump(tokenized_texts, f)\n",
    "    with open(dictionary_file, \"wb\") as f:\n",
    "        pickle.dump(dictionary, f)\n",
    "    with open(texts_clean_file, \"wb\") as f:\n",
    "        pickle.dump(texts_clean, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "071edf90-3c27-48d8-994e-3dbf35c2803c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise training and evaluation logs \n",
    "training_log_df = pd.read_csv(\"training_log.csv\")\n",
    "evaluation_log_df = None\n",
    "\n",
    "# load evaluation into df\n",
    "if os.path.exists(MODEL_EVALUATION_LOG):\n",
    "    evaluation_log_df = pd.read_csv(MODEL_EVALUATION_LOG)\n",
    "else:\n",
    "    evaluation_log_df = pd.DataFrame(columns=[\"model_name\",\n",
    "                                              \"embedding_model\",\n",
    "                                              \"n_neighbors\",\n",
    "                                              \"n_components\",\n",
    "                                              \"min_cluster_size\",\n",
    "                                              \"train_time\", \n",
    "                                              \"monogram_cv\", \n",
    "                                              \"multigram_cv\", \n",
    "                                              \"eval_time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41099078-b4bc-45bc-8fa5-24db41aebaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all-mpnet-base-v2_5_5_5 in 82.06482124328613 : 0.3169934458652391, 0.398788266411994\n",
      "all-mpnet-base-v2_5_5_10 in 40.29389977455139 : 0.31667664430618864, 0.40297466225762124\n",
      "all-mpnet-base-v2_5_5_15 in 32.284870624542236 : 0.30475759592027607, 0.3765922846713736\n",
      "all-mpnet-base-v2_5_5_20 in 32.33454966545105 : 0.340007685369821, 0.38996022016882687\n",
      "all-mpnet-base-v2_5_5_30 in 28.28744602203369 : 0.3743542628814256, 0.44481123297707165\n",
      "all-mpnet-base-v2_5_5_50 in 29.418080806732178 : 0.41043809380653284, 0.5373819755575944\n",
      "all-mpnet-base-v2_5_10_5 in 82.65697431564331 : 0.30543355980651477, 0.40334723629343955\n",
      "all-mpnet-base-v2_5_10_10 in 41.00922417640686 : 0.30996029891519933, 0.40041722357398807\n",
      "all-mpnet-base-v2_5_10_15 in 33.86795234680176 : 0.30857550056605537, 0.3873626238519822\n",
      "all-mpnet-base-v2_5_10_20 in 30.947529315948486 : 0.33113750782288054, 0.39218163386109217\n",
      "all-mpnet-base-v2_5_10_30 in 28.657154321670532 : 0.3477115339440953, 0.45471484532131595\n",
      "all-mpnet-base-v2_5_10_50 in 28.405856370925903 : 0.35741545367380945, 0.41998579889327375\n",
      "all-mpnet-base-v2_5_15_5 in 84.78447079658508 : 0.3230376511795886, 0.39560997418867366\n",
      "all-mpnet-base-v2_5_15_10 in 39.05079698562622 : 0.32001214067709294, 0.4025854287123467\n",
      "all-mpnet-base-v2_5_15_15 in 34.19217872619629 : 0.33358956291977265, 0.3966209353628553\n",
      "all-mpnet-base-v2_5_15_20 in 31.955609560012817 : 0.357570981391776, 0.39484972571626176\n",
      "all-mpnet-base-v2_5_15_30 in 28.315685749053955 : 0.3484848076107359, 0.46622221761077215\n",
      "all-mpnet-base-v2_5_15_50 in 27.82878875732422 : 0.36896196004659615, 0.4485109766820503\n",
      "all-mpnet-base-v2_5_20_5 in 80.64545154571533 : 0.32317454043748467, 0.3975214279397288\n",
      "all-mpnet-base-v2_5_20_10 in 40.602656841278076 : 0.3103736796279737, 0.39966939566817833\n",
      "all-mpnet-base-v2_5_20_15 in 30.993943452835083 : 0.3420931371029685, 0.3966165443418359\n",
      "all-mpnet-base-v2_5_20_20 in 29.106893301010132 : 0.34084412547973003, 0.4036849186441976\n",
      "all-mpnet-base-v2_5_20_30 in 27.812332153320312 : 0.390593177812055, 0.47109137934314427\n",
      "all-mpnet-base-v2_5_20_50 in 27.65272855758667 : 0.3501654836823433, 0.4531397876357007\n",
      "all-mpnet-base-v2_5_25_5 in 71.07671976089478 : 0.32453611048015885, 0.39559009392707983\n",
      "all-mpnet-base-v2_5_25_10 in 36.70970153808594 : 0.3234145676333711, 0.39270213772244517\n",
      "all-mpnet-base-v2_5_25_15 in 31.196679830551147 : 0.32756593187759453, 0.3898158190054924\n",
      "all-mpnet-base-v2_5_25_20 in 29.936219930648804 : 0.3260536499706845, 0.39837409783890054\n",
      "all-mpnet-base-v2_5_25_30 in 29.16089630126953 : 0.35074231564291564, 0.4553049356237576\n",
      "all-mpnet-base-v2_5_25_50 in 28.200275421142578 : 0.33622306138139285, 0.45820877488186684\n",
      "all-mpnet-base-v2_10_5_5 in 57.09225416183472 : 0.3229944064693881, 0.39563415452347583\n",
      "all-mpnet-base-v2_10_5_10 in 40.049373388290405 : 0.2998899178723328, 0.40119565212558367\n",
      "all-mpnet-base-v2_10_5_15 in 42.68775129318237 : 0.3282914357862896, 0.42126107092278886\n",
      "all-mpnet-base-v2_10_5_20 in 41.192338943481445 : 0.3390605275521653, 0.3903741553215452\n",
      "all-mpnet-base-v2_10_5_30 in 39.11776900291443 : 0.32033357091813036, 0.4240639266947377\n",
      "all-mpnet-base-v2_10_5_50 in 38.726635217666626 : 0.35487866869151563, 0.42846146809170854\n",
      "all-mpnet-base-v2_10_10_5 in 78.73182892799377 : 0.32607051810146565, 0.3927588483852189\n",
      "all-mpnet-base-v2_10_10_10 in 47.32964038848877 : 0.3285637342362615, 0.3984149205510995\n",
      "all-mpnet-base-v2_10_10_15 in 34.482449769973755 : 0.33580626434158134, 0.3951874746785042\n",
      "all-mpnet-base-v2_10_10_20 in 30.07692575454712 : 0.32965774410678167, 0.38337177419115503\n",
      "all-mpnet-base-v2_10_10_30 in 29.97694969177246 : 0.3691188650957242, 0.41812610614325046\n",
      "all-mpnet-base-v2_10_10_50 in 28.90942358970642 : 0.3485165555831227, 0.3883119612543847\n",
      "all-mpnet-base-v2_10_15_5 in 62.59057307243347 : 0.32243118207293625, 0.39874437709373\n",
      "all-mpnet-base-v2_10_15_10 in 36.13682699203491 : 0.323123052731012, 0.40599389451155576\n",
      "all-mpnet-base-v2_10_15_15 in 33.056150913238525 : 0.3339600894251551, 0.3995627986214921\n",
      "all-mpnet-base-v2_10_15_20 in 32.080517053604126 : 0.33837058320503716, 0.3904540277562704\n",
      "all-mpnet-base-v2_10_15_30 in 30.8505916595459 : 0.33527501093763684, 0.44107594490607405\n",
      "all-mpnet-base-v2_10_15_50 in 30.402162551879883 : 0.37325647670206563, 0.4901534122744757\n",
      "all-mpnet-base-v2_10_20_5 in 61.84615683555603 : 0.3287949539827665, 0.4009538492648703\n",
      "all-mpnet-base-v2_10_20_10 in 36.60945510864258 : 0.3245893416665946, 0.402014489437933\n",
      "all-mpnet-base-v2_10_20_15 in 33.24390649795532 : 0.34297574522411794, 0.3784971266215291\n",
      "all-mpnet-base-v2_10_20_20 in 30.92022132873535 : 0.3789532863381509, 0.4384732728173378\n",
      "all-mpnet-base-v2_10_20_30 in 30.859495162963867 : 0.3509170755201654, 0.41806181901678086\n",
      "all-mpnet-base-v2_10_20_50 in 30.53246784210205 : 0.3539575881270411, 0.4733065242881433\n",
      "all-mpnet-base-v2_10_25_5 in 60.98290991783142 : 0.3271713755524981, 0.4028324430591236\n",
      "all-mpnet-base-v2_10_25_10 in 37.18986415863037 : 0.3309731081004082, 0.4110750636441874\n",
      "all-mpnet-base-v2_10_25_15 in 32.773000717163086 : 0.33820427593706043, 0.4115835110111422\n",
      "all-mpnet-base-v2_10_25_20 in 31.647767305374146 : 0.35837980928040647, 0.4157026082895605\n",
      "all-mpnet-base-v2_10_25_30 in 30.706573009490967 : 0.38226322427500237, 0.39740826120529443\n",
      "all-mpnet-base-v2_10_25_50 in 30.574772834777832 : 0.3288949292339295, 0.42887424875135605\n",
      "all-mpnet-base-v2_15_5_5 in 54.21344470977783 : 0.32961841703556194, 0.38800288336067895\n",
      "all-mpnet-base-v2_15_5_10 in 35.687846660614014 : 0.3100876560781315, 0.4025420437233306\n",
      "all-mpnet-base-v2_15_5_15 in 32.27317929267883 : 0.34327752059953076, 0.4047546849501282\n",
      "all-mpnet-base-v2_15_5_20 in 31.4393310546875 : 0.34531021724482114, 0.40091984214541326\n",
      "all-mpnet-base-v2_15_5_30 in 31.234931230545044 : 0.3666601511208304, 0.421216038595936\n",
      "all-mpnet-base-v2_15_5_50 in 30.779605627059937 : 0.3352902117830848, 0.41216442929297753\n",
      "all-mpnet-base-v2_15_10_5 in 53.12314581871033 : 0.3182274364879109, 0.40049910062985644\n",
      "all-mpnet-base-v2_15_10_10 in 35.41628837585449 : 0.3249082019857617, 0.41314890778030383\n",
      "all-mpnet-base-v2_15_10_15 in 32.328951597213745 : 0.33242075152838785, 0.3960256298184013\n",
      "all-mpnet-base-v2_15_10_20 in 31.502115964889526 : 0.3413399160928085, 0.4131386568334335\n",
      "all-mpnet-base-v2_15_10_30 in 30.838324546813965 : 0.3640496724755224, 0.3990848898585138\n",
      "all-mpnet-base-v2_15_10_50 in 30.953453063964844 : 0.30817831111446725, 0.44524371458425255\n",
      "all-mpnet-base-v2_15_15_5 in 57.39069962501526 : 0.3221943339162339, 0.40477631020636445\n",
      "all-mpnet-base-v2_15_15_10 in 35.33820462226868 : 0.3323129371686346, 0.40862499319525425\n",
      "all-mpnet-base-v2_15_15_15 in 33.5218231678009 : 0.32967434447765365, 0.401661011093571\n",
      "all-mpnet-base-v2_15_15_20 in 31.835258960723877 : 0.35792919230245873, 0.3979890617311677\n",
      "all-mpnet-base-v2_15_15_30 in 31.010976791381836 : 0.3548707596509085, 0.414783528801236\n",
      "all-mpnet-base-v2_15_15_50 in 31.27158808708191 : 0.32035255845611393, 0.44190586882261945\n",
      "all-mpnet-base-v2_15_20_5 in 56.67399883270264 : 0.31729036879765854, 0.39597629013043045\n",
      "all-mpnet-base-v2_15_20_10 in 35.573052167892456 : 0.30902441492753924, 0.41495739450250657\n",
      "all-mpnet-base-v2_15_20_15 in 31.90219020843506 : 0.3178218615841963, 0.40144774356212803\n",
      "all-mpnet-base-v2_15_20_20 in 31.595229864120483 : 0.3583541848897844, 0.41345767263543554\n",
      "all-mpnet-base-v2_15_20_30 in 31.287907600402832 : 0.30489922817326787, 0.41755104095407347\n",
      "all-mpnet-base-v2_15_20_50 in 30.840620517730713 : 0.34559779342720165, 0.4282949007566824\n",
      "all-mpnet-base-v2_15_25_5 in 57.86447095870972 : 0.3236738027210437, 0.39694337763639365\n",
      "all-mpnet-base-v2_15_25_10 in 36.11758470535278 : 0.3215387590340561, 0.409126922775504\n",
      "all-mpnet-base-v2_15_25_15 in 32.33614754676819 : 0.3379215233564802, 0.4113024049325159\n",
      "all-mpnet-base-v2_15_25_20 in 31.520493507385254 : 0.33858316284346535, 0.4044191361727315\n",
      "all-mpnet-base-v2_15_25_30 in 30.894296169281006 : 0.3565717725015227, 0.4443098783962587\n",
      "all-mpnet-base-v2_15_25_50 in 31.14382028579712 : 0.3125970817082492, 0.4234971247031895\n",
      "all-mpnet-base-v2_25_5_5 in 49.41985893249512 : 0.32405294643482585, 0.40342671604084684\n",
      "all-mpnet-base-v2_25_5_10 in 34.05887508392334 : 0.32718172098922105, 0.39796107516095697\n",
      "all-mpnet-base-v2_25_5_15 in 32.26847696304321 : 0.3340972282278072, 0.40369866338856497\n",
      "all-mpnet-base-v2_25_5_20 in 31.36908531188965 : 0.3235441001699736, 0.41298180024735853\n",
      "all-mpnet-base-v2_25_5_30 in 31.08930015563965 : 0.3674184491456247, 0.4173077402497796\n",
      "all-mpnet-base-v2_25_5_50 in 30.735567092895508 : 0.3782258553799487, 0.4124151443097116\n",
      "all-mpnet-base-v2_25_10_5 in 46.04270362854004 : 0.3298885873180178, 0.3882576794829964\n",
      "all-mpnet-base-v2_25_10_10 in 34.92336702346802 : 0.3366289444503882, 0.3983242282298029\n",
      "all-mpnet-base-v2_25_10_15 in 32.19433784484863 : 0.3284008517240291, 0.420553558844441\n",
      "all-mpnet-base-v2_25_10_20 in 31.431434869766235 : 0.3340745264644038, 0.39666365495158995\n",
      "all-mpnet-base-v2_25_10_30 in 31.160133123397827 : 0.3798681041891848, 0.39684878196054607\n",
      "all-mpnet-base-v2_25_10_50 in 31.217172384262085 : 0.3115028609157111, 0.40128628192475263\n",
      "all-mpnet-base-v2_25_15_5 in 49.01835513114929 : 0.32357004172964965, 0.40286233346843897\n",
      "all-mpnet-base-v2_25_15_10 in 34.30775856971741 : 0.32264683166023506, 0.4056564396878692\n",
      "all-mpnet-base-v2_25_15_15 in 32.04062342643738 : 0.3446048490053059, 0.41495168531107074\n",
      "all-mpnet-base-v2_25_15_20 in 31.587209701538086 : 0.3571195837982056, 0.39630004660702617\n",
      "all-mpnet-base-v2_25_15_30 in 31.262232065200806 : 0.3268295152541261, 0.39634982763614157\n",
      "all-mpnet-base-v2_25_15_50 in 30.579469919204712 : 0.391851959908628, 0.4901534122744757\n",
      "all-mpnet-base-v2_25_20_5 in 49.88926339149475 : 0.3198000853065387, 0.40401961449895707\n",
      "all-mpnet-base-v2_25_20_10 in 34.11918616294861 : 0.33855786483018524, 0.3960596735355957\n",
      "all-mpnet-base-v2_25_20_15 in 32.23148155212402 : 0.35886609527819074, 0.42496769908198473\n",
      "all-mpnet-base-v2_25_20_20 in 31.684462785720825 : 0.3630653131849642, 0.3948734778217853\n",
      "all-mpnet-base-v2_25_20_30 in 31.02241849899292 : 0.31874814719318795, 0.402221638523569\n",
      "all-mpnet-base-v2_25_20_50 in 31.103384733200073 : 0.34383448984977183, 0.3765309888835376\n",
      "all-mpnet-base-v2_25_25_5 in 49.160162687301636 : 0.33131484925466087, 0.39368368824408456\n",
      "all-mpnet-base-v2_25_25_10 in 33.394137144088745 : 0.32567601102151306, 0.3978751567517457\n",
      "all-mpnet-base-v2_25_25_15 in 32.08771109580994 : 0.34857074342690253, 0.4026252784378\n",
      "all-mpnet-base-v2_25_25_20 in 31.547807931900024 : 0.3298503535693593, 0.39599460841152756\n",
      "all-mpnet-base-v2_25_25_30 in 31.152083158493042 : 0.3380152568292645, 0.38980603224696625\n",
      "all-mpnet-base-v2_25_25_50 in 30.58178400993347 : 0.33247041898345653, 0.3725943147003827\n",
      "all-mpnet-base-v2_50_5_5 in 39.858100175857544 : 0.32833425747338285, 0.40117695658426267\n",
      "all-mpnet-base-v2_50_5_10 in 32.833008766174316 : 0.3220436909959497, 0.4147626917554594\n",
      "all-mpnet-base-v2_50_5_15 in 32.143635511398315 : 0.34300121546357937, 0.4100141323549354\n",
      "all-mpnet-base-v2_50_5_20 in 31.548151969909668 : 0.3272808545424698, 0.3942223014163394\n",
      "all-mpnet-base-v2_50_5_30 in 31.258183240890503 : 0.36291293576680367, 0.3756476795602649\n",
      "all-mpnet-base-v2_50_5_50 in 31.028742790222168 : 0.357744053467398, 0.36774248483667166\n",
      "all-mpnet-base-v2_50_10_5 in 39.25836968421936 : 0.3326309783918545, 0.38983430239843897\n",
      "all-mpnet-base-v2_50_10_10 in 38.038347244262695 : 0.3237282077119432, 0.42377684996781806\n",
      "all-mpnet-base-v2_50_10_15 in 33.374852418899536 : 0.32822247119938275, 0.42335617666649566\n",
      "all-mpnet-base-v2_50_10_20 in 37.234914779663086 : 0.36193478594656325, 0.3806126459343609\n",
      "all-mpnet-base-v2_50_10_30 in 36.187132835388184 : 0.34978688959800597, 0.39420797119026174\n",
      "all-mpnet-base-v2_50_10_50 in 39.75228142738342 : 0.391851959908628, 0.4901534122744757\n",
      "all-mpnet-base-v2_50_15_5 in 47.41743016242981 : 0.3365190206977834, 0.40813506519538983\n",
      "all-mpnet-base-v2_50_15_10 in 39.58066010475159 : 0.3421499069291056, 0.38760290114832036\n",
      "all-mpnet-base-v2_50_15_15 in 34.64309215545654 : 0.33982025533329574, 0.4110220525909677\n",
      "all-mpnet-base-v2_50_15_20 in 35.148478507995605 : 0.33348522875421066, 0.40033919850732896\n",
      "all-mpnet-base-v2_50_15_30 in 30.563832759857178 : 0.3525469172030266, 0.37504920575728884\n",
      "all-mpnet-base-v2_50_15_50 in 31.388039112091064 : 0.3272545853737786, 0.3760653598562561\n",
      "all-mpnet-base-v2_50_20_5 in 45.062729597091675 : 0.32633938362669335, 0.3953179336196539\n",
      "all-mpnet-base-v2_50_20_10 in 30.827999591827393 : 0.31359000672753823, 0.4126028066150304\n",
      "all-mpnet-base-v2_50_20_15 in 30.13604426383972 : 0.3334946827334922, 0.40757547903324953\n",
      "all-mpnet-base-v2_50_20_20 in 33.01652932167053 : 0.3215279506292441, 0.39246415064732726\n",
      "all-mpnet-base-v2_50_20_30 in 32.81478404998779 : 0.35907090536453695, 0.3714070214906429\n",
      "all-mpnet-base-v2_50_20_50 in 32.40558910369873 : 0.35463991655973237, 0.4027972060037935\n",
      "all-mpnet-base-v2_50_25_5 in 41.61031913757324 : 0.33356436162148234, 0.38613120463469974\n",
      "all-mpnet-base-v2_50_25_10 in 30.973812103271484 : 0.31065718472546006, 0.4061656707870902\n",
      "all-mpnet-base-v2_50_25_15 in 30.01597309112549 : 0.3440978895073391, 0.4194081096602697\n",
      "all-mpnet-base-v2_50_25_20 in 29.602161169052124 : 0.35176795043937364, 0.4025442506956637\n",
      "all-mpnet-base-v2_50_25_30 in 29.152645349502563 : 0.3351527568898308, 0.34973442341375444\n",
      "all-mpnet-base-v2_50_25_50 in 29.13463592529297 : 0.34251238209421897, 0.41634441035348224\n",
      "all-MiniLM-L6-v2_5_5_5 in 81.64024567604065 : 0.3260840103602977, 0.3963403370710737\n",
      "all-MiniLM-L6-v2_5_5_10 in 38.4860475063324 : 0.30380457931996313, 0.3953064174124715\n",
      "all-MiniLM-L6-v2_5_5_15 in 35.98428559303284 : 0.3253243952835478, 0.41135784895720706\n",
      "all-MiniLM-L6-v2_5_5_20 in 31.394189834594727 : 0.3566742638375735, 0.39210936381555767\n",
      "all-MiniLM-L6-v2_5_5_30 in 30.39091467857361 : 0.33458270271372054, 0.40372665989203826\n",
      "all-MiniLM-L6-v2_5_5_50 in 29.300637006759644 : 0.31228953675244003, 0.3934958033199148\n",
      "all-MiniLM-L6-v2_5_10_5 in 80.92100381851196 : 0.31796960483103914, 0.39423664613497933\n",
      "all-MiniLM-L6-v2_5_10_10 in 36.51546931266785 : 0.30421116917862245, 0.3944045827301421\n",
      "all-MiniLM-L6-v2_5_10_15 in 30.98663330078125 : 0.31954906780012793, 0.4066958068229157\n",
      "all-MiniLM-L6-v2_5_10_20 in 29.77989101409912 : 0.33472501831097523, 0.40588059596271137\n",
      "all-MiniLM-L6-v2_5_10_30 in 28.938918352127075 : 0.33049478072158633, 0.39823481206876676\n",
      "all-MiniLM-L6-v2_5_10_50 in 28.10245156288147 : 0.35946564006056486, 0.389976464459029\n",
      "all-MiniLM-L6-v2_5_15_5 in 70.06293630599976 : 0.3264491545743021, 0.39479531254293254\n",
      "all-MiniLM-L6-v2_5_15_10 in 35.82792353630066 : 0.313331290126817, 0.3883801718732397\n",
      "all-MiniLM-L6-v2_5_15_15 in 32.23401856422424 : 0.3368770721186867, 0.4048647544392389\n",
      "all-MiniLM-L6-v2_5_15_20 in 30.36335062980652 : 0.343441733222553, 0.4022362829318429\n",
      "all-MiniLM-L6-v2_5_15_30 in 29.466867685317993 : 0.33761465310352873, 0.3838338029640119\n",
      "all-MiniLM-L6-v2_5_15_50 in 28.3870530128479 : 0.3438820418085014, 0.40621209688605797\n",
      "all-MiniLM-L6-v2_5_20_5 in 79.69795370101929 : 0.3209326043787207, 0.3890806882419397\n",
      "all-MiniLM-L6-v2_5_20_10 in 35.676194190979004 : 0.3261358107941977, 0.4070842162436196\n",
      "all-MiniLM-L6-v2_5_20_15 in 31.50828719139099 : 0.33968518844124207, 0.4081916500010075\n",
      "all-MiniLM-L6-v2_5_20_20 in 29.836983919143677 : 0.34533985414401436, 0.4068537119607294\n",
      "all-MiniLM-L6-v2_5_20_30 in 28.35771155357361 : 0.3290288827785803, 0.39894762716956456\n",
      "all-MiniLM-L6-v2_5_20_50 in 28.049547910690308 : 0.357897461358391, 0.3851490512169506\n",
      "all-MiniLM-L6-v2_5_25_5 in 78.98084712028503 : 0.3256208144286772, 0.40158983785448343\n",
      "all-MiniLM-L6-v2_5_25_10 in 35.652719020843506 : 0.32201087071201134, 0.39840494776658963\n",
      "all-MiniLM-L6-v2_5_25_15 in 31.187715768814087 : 0.33656355203837174, 0.4207765741774145\n",
      "all-MiniLM-L6-v2_5_25_20 in 29.750626802444458 : 0.32349036546781684, 0.4029493413645904\n",
      "all-MiniLM-L6-v2_5_25_30 in 29.725905418395996 : 0.3310079548673109, 0.3868160568119508\n",
      "all-MiniLM-L6-v2_5_25_50 in 28.14073634147644 : 0.330996844189339, 0.3903071083402322\n",
      "all-MiniLM-L6-v2_10_5_5 in 56.18833661079407 : 0.32376465061619547, 0.3962179318770368\n",
      "all-MiniLM-L6-v2_10_5_10 in 33.11013698577881 : 0.31309914694897584, 0.396413277300032\n",
      "all-MiniLM-L6-v2_10_5_15 in 30.500089406967163 : 0.3234300097312251, 0.3869057857144607\n",
      "all-MiniLM-L6-v2_10_5_20 in 29.69765043258667 : 0.32817659765805274, 0.3919190835779088\n",
      "all-MiniLM-L6-v2_10_5_30 in 27.96575403213501 : 0.3355892417334081, 0.49015341227447573\n",
      "all-MiniLM-L6-v2_10_5_50 in 28.22835874557495 : 0.3743107954070963, 0.47569557228580667\n",
      "all-MiniLM-L6-v2_10_10_5 in 55.54647421836853 : 0.31907576908373253, 0.39803270237125044\n",
      "all-MiniLM-L6-v2_10_10_10 in 32.859468936920166 : 0.33064480885628894, 0.3917726731846429\n",
      "all-MiniLM-L6-v2_10_10_15 in 30.60859751701355 : 0.32512264965255544, 0.4003340229305861\n",
      "all-MiniLM-L6-v2_10_10_20 in 28.548370122909546 : 0.3545525306374279, 0.42234589195897065\n",
      "all-MiniLM-L6-v2_10_10_30 in 28.359089851379395 : 0.33874856921800495, 0.39517130477806034\n",
      "all-MiniLM-L6-v2_10_10_50 in 28.1393620967865 : 0.33512244337222297, 0.384622687509889\n",
      "all-MiniLM-L6-v2_10_15_5 in 54.577996015548706 : 0.317547784630892, 0.3937315495232817\n",
      "all-MiniLM-L6-v2_10_15_10 in 32.18182015419006 : 0.31801131844173064, 0.3899938927977831\n",
      "all-MiniLM-L6-v2_10_15_15 in 30.048582315444946 : 0.31956230725193796, 0.39312405079615625\n",
      "all-MiniLM-L6-v2_10_15_20 in 29.344951629638672 : 0.3344867369406275, 0.40735061707389014\n",
      "all-MiniLM-L6-v2_10_15_30 in 28.507357120513916 : 0.34040197029142943, 0.4073082601060075\n",
      "all-MiniLM-L6-v2_10_15_50 in 34.39053964614868 : 0.3435308125444241, 0.39360529372607594\n",
      "all-MiniLM-L6-v2_10_20_5 in 55.68828630447388 : 0.33498556782764366, 0.3980229410116304\n",
      "all-MiniLM-L6-v2_10_20_10 in 33.468405961990356 : 0.3125079476462717, 0.3894749181441873\n",
      "all-MiniLM-L6-v2_10_20_15 in 29.984952449798584 : 0.33356671982031627, 0.41254773734917427\n",
      "all-MiniLM-L6-v2_10_20_20 in 29.000631093978882 : 0.320705867135714, 0.42173724720918165\n",
      "all-MiniLM-L6-v2_10_20_30 in 28.405791759490967 : 0.3847862616677572, 0.4210322333770685\n",
      "all-MiniLM-L6-v2_10_20_50 in 27.92740511894226 : 0.33928171111978345, 0.39359105606083816\n",
      "all-MiniLM-L6-v2_10_25_5 in 49.10931992530823 : 0.33383183845021436, 0.39943177244290806\n",
      "all-MiniLM-L6-v2_10_25_10 in 32.394158124923706 : 0.32161323051768526, 0.39811794073134893\n",
      "all-MiniLM-L6-v2_10_25_15 in 30.206329107284546 : 0.32711202207602946, 0.39412039841569596\n",
      "all-MiniLM-L6-v2_10_25_20 in 28.15619444847107 : 0.361276854439541, 0.4170387242231603\n",
      "all-MiniLM-L6-v2_10_25_30 in 28.094493865966797 : 0.3409324078648017, 0.3968978225562116\n",
      "all-MiniLM-L6-v2_10_25_50 in 28.06708574295044 : 0.3711326334898247, 0.38247191244934414\n",
      "all-MiniLM-L6-v2_15_5_5 in 45.47116422653198 : 0.32829517186856155, 0.3995294716694569\n",
      "all-MiniLM-L6-v2_15_5_10 in 32.12593388557434 : 0.3327303931569258, 0.40314762941681104\n",
      "all-MiniLM-L6-v2_15_5_15 in 30.01645588874817 : 0.3380228732552232, 0.39077615790707915\n",
      "all-MiniLM-L6-v2_15_5_20 in 28.93808126449585 : 0.34293570134343804, 0.40404541860848214\n",
      "all-MiniLM-L6-v2_15_5_30 in 28.155861616134644 : 0.32360750983590597, 0.3986372699459546\n",
      "all-MiniLM-L6-v2_15_5_50 in 27.82744789123535 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_15_10_5 in 45.44290733337402 : 0.3219872836167151, 0.3978729603427964\n",
      "all-MiniLM-L6-v2_15_10_10 in 31.458555698394775 : 0.3265780331893529, 0.40882927455399287\n",
      "all-MiniLM-L6-v2_15_10_15 in 30.12401294708252 : 0.3333614771229255, 0.40240815862216955\n",
      "all-MiniLM-L6-v2_15_10_20 in 29.229764699935913 : 0.32922246377270803, 0.39971919912803533\n",
      "all-MiniLM-L6-v2_15_10_30 in 28.672975301742554 : 0.33163796193124245, 0.362996720654738\n",
      "all-MiniLM-L6-v2_15_10_50 in 27.67544436454773 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_15_15_5 in 45.227933406829834 : 0.32353957284341983, 0.39592325814482376\n",
      "all-MiniLM-L6-v2_15_15_10 in 32.18760919570923 : 0.30405026074707314, 0.40545783917992134\n",
      "all-MiniLM-L6-v2_15_15_15 in 29.87294316291809 : 0.32666153217668475, 0.40001805237646787\n",
      "all-MiniLM-L6-v2_15_15_20 in 28.996505975723267 : 0.35677658740062895, 0.4108211922126597\n",
      "all-MiniLM-L6-v2_15_15_30 in 27.890115976333618 : 0.3355892417334081, 0.49015341227447573\n",
      "all-MiniLM-L6-v2_15_15_50 in 28.489094734191895 : 0.32614039884332474, 0.3975495322339987\n",
      "all-MiniLM-L6-v2_15_20_5 in 43.985119104385376 : 0.3280494007912752, 0.40824537133789435\n",
      "all-MiniLM-L6-v2_15_20_10 in 31.682313442230225 : 0.3266369704931611, 0.4053238517885407\n",
      "all-MiniLM-L6-v2_15_20_15 in 29.96950602531433 : 0.3170756953073562, 0.40075403499118284\n",
      "all-MiniLM-L6-v2_15_20_20 in 28.858208656311035 : 0.3405321104095053, 0.4008196694194814\n",
      "all-MiniLM-L6-v2_15_20_30 in 28.363000631332397 : 0.3355892417334081, 0.49015341227447573\n",
      "all-MiniLM-L6-v2_15_20_50 in 27.87736439704895 : 0.3355892417334081, 0.49015341227447573\n",
      "all-MiniLM-L6-v2_15_25_5 in 46.18943643569946 : 0.31527977532434903, 0.4017496848777668\n",
      "all-MiniLM-L6-v2_15_25_10 in 31.654799938201904 : 0.32116389553219843, 0.40966542944286316\n",
      "all-MiniLM-L6-v2_15_25_15 in 30.07469630241394 : 0.32792506969916285, 0.3991091986704603\n",
      "all-MiniLM-L6-v2_15_25_20 in 29.15598154067993 : 0.33339495159742516, 0.3983734673945724\n",
      "all-MiniLM-L6-v2_15_25_30 in 28.04845118522644 : 0.3228512376744235, 0.38121916342285994\n",
      "all-MiniLM-L6-v2_15_25_50 in 28.246919870376587 : 0.3314785395569718, 0.3747952272150807\n",
      "all-MiniLM-L6-v2_25_5_5 in 39.733221769332886 : 0.3194580612846761, 0.3851065716735685\n",
      "all-MiniLM-L6-v2_25_5_10 in 30.906192541122437 : 0.3297015487475991, 0.4157833692576338\n",
      "all-MiniLM-L6-v2_25_5_15 in 29.96781826019287 : 0.3329346788359295, 0.40476738528486556\n",
      "all-MiniLM-L6-v2_25_5_20 in 28.87749171257019 : 0.3385806370016121, 0.38439587091626876\n",
      "all-MiniLM-L6-v2_25_5_30 in 27.90577983856201 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_5_50 in 27.818703174591064 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_10_5 in 39.80955386161804 : 0.3260296440912927, 0.39309528950007955\n",
      "all-MiniLM-L6-v2_25_10_10 in 31.876294136047363 : 0.33842842486475644, 0.4030447840299674\n",
      "all-MiniLM-L6-v2_25_10_15 in 29.66777467727661 : 0.34766122469634536, 0.40176393038268604\n",
      "all-MiniLM-L6-v2_25_10_20 in 29.015280723571777 : 0.3275804949010148, 0.3922210596722445\n",
      "all-MiniLM-L6-v2_25_10_30 in 27.854173183441162 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_10_50 in 27.767255067825317 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_15_5 in 39.89149880409241 : 0.3258995965835627, 0.39668759323458386\n",
      "all-MiniLM-L6-v2_25_15_10 in 31.381638288497925 : 0.32501594786578814, 0.4052218721530616\n",
      "all-MiniLM-L6-v2_25_15_15 in 29.71925711631775 : 0.35634173768804794, 0.40233307868125606\n",
      "all-MiniLM-L6-v2_25_15_20 in 29.012568950653076 : 0.3328612855090614, 0.40226505491787395\n",
      "all-MiniLM-L6-v2_25_15_30 in 28.01529860496521 : 0.3355892417334081, 0.49015341227447573\n",
      "all-MiniLM-L6-v2_25_15_50 in 27.858275413513184 : 0.3355892417334081, 0.49015341227447573\n",
      "all-MiniLM-L6-v2_25_20_5 in 38.66082239151001 : 0.3240230284230826, 0.39119165296077507\n",
      "all-MiniLM-L6-v2_25_20_10 in 31.24511432647705 : 0.3187765001596782, 0.39724613078900267\n",
      "all-MiniLM-L6-v2_25_20_15 in 29.593245029449463 : 0.3503774795988952, 0.4080120053166874\n",
      "all-MiniLM-L6-v2_25_20_20 in 27.868144512176514 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_20_30 in 27.85545039176941 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_20_50 in 27.69412326812744 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_25_5 in 39.79864430427551 : 0.32734855061186746, 0.387668863529882\n",
      "all-MiniLM-L6-v2_25_25_10 in 31.109209299087524 : 0.32205527677036305, 0.3909805384001922\n",
      "all-MiniLM-L6-v2_25_25_15 in 29.79581069946289 : 0.3283014861588006, 0.4129763008226143\n",
      "all-MiniLM-L6-v2_25_25_20 in 27.732133626937866 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_25_30 in 27.80122137069702 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_25_25_50 in 27.827434062957764 : 0.3355892417334081, 0.4901534122744756\n",
      "all-MiniLM-L6-v2_50_5_5 in 35.0308678150177 : 0.33448043791913684, 0.40145543644901416\n",
      "all-MiniLM-L6-v2_50_5_10 in 30.33816385269165 : 0.3386536361683259, 0.40575003398555526\n",
      "all-MiniLM-L6-v2_50_5_15 in 27.799203634262085 : 0.4876317322191586, 0.5798887185138518\n",
      "all-MiniLM-L6-v2_50_5_20 in 27.8265221118927 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_5_30 in 27.88639259338379 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_5_50 in 28.027424812316895 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_10_5 in 35.67137956619263 : 0.34065203983562414, 0.38457039816489963\n",
      "all-MiniLM-L6-v2_50_10_10 in 30.458850383758545 : 0.32476861429904563, 0.4152043446434062\n",
      "all-MiniLM-L6-v2_50_10_15 in 27.90237069129944 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_10_20 in 27.73992395401001 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_10_30 in 27.875473022460938 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_10_50 in 28.615902185440063 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_15_5 in 35.20634865760803 : 0.33952422925510584, 0.38591137764198347\n",
      "all-MiniLM-L6-v2_50_15_10 in 31.335421800613403 : 0.3388530046738393, 0.4265310860468671\n",
      "all-MiniLM-L6-v2_50_15_15 in 28.089372634887695 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_15_20 in 28.343007564544678 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_15_30 in 27.782877683639526 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_15_50 in 28.081473350524902 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_20_5 in 35.31183338165283 : 0.35118000603110106, 0.4001280953623098\n",
      "all-MiniLM-L6-v2_50_20_10 in 30.183370113372803 : 0.3403192460240794, 0.41293463715822554\n",
      "all-MiniLM-L6-v2_50_20_15 in 27.664286851882935 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_20_20 in 28.047932863235474 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_20_30 in 27.92097043991089 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_20_50 in 27.983919143676758 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_25_5 in 35.264832973480225 : 0.33923366059643423, 0.4089783094233749\n",
      "all-MiniLM-L6-v2_50_25_10 in 30.28886389732361 : 0.3132548381748205, 0.40377135977886963\n",
      "all-MiniLM-L6-v2_50_25_15 in 27.765626192092896 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_25_20 in 28.23798108100891 : 0.3355892417334081, 0.5930945415265142\n",
      "all-MiniLM-L6-v2_50_25_30 in 28.250291347503662 : 0.3642914567969134, 0.4257113250175574\n",
      "all-MiniLM-L6-v2_50_25_50 in 27.876166343688965 : 0.3355892417334081, 0.5930945415265142\n"
     ]
    }
   ],
   "source": [
    "for _, row in training_log_df.iterrows():\n",
    "    model_name = row[\"model_name\"]\n",
    "    train_time = row[\"train_time\"]\n",
    "\n",
    "    embedding_model_name,n_neighbors,n_components,min_cluster_size = parse_model_name(model_name)\n",
    "    \n",
    "    # Skip evaluation if evaluated before\n",
    "    if model_name in evaluation_log_df[\"model_name\"].values:\n",
    "        print(f\"{model_name} has already evaluated.\")\n",
    "        continue\n",
    "    \n",
    "    model = os.path.join(MODEL_FOLDER,model_name)\n",
    "    embedding_model = SentenceTransformer(embedding_model_name,device=DEVICE) # Hardcoded for quick test, will change later\n",
    "\n",
    "    start_time = time.time()\n",
    "    monogram_cv = monogram_coherence_score(model,embedding_model,tokenized_texts,dictionary)\n",
    "    multigram_cv = multigram_coherence_score(model,embedding_model,tokenized_texts,dictionary,texts_clean)\n",
    "    end_time = time.time()\n",
    "    eval_time = end_time-start_time\n",
    "\n",
    "    print(f\"{model_name} in {eval_time} : {monogram_cv}, {multigram_cv}\")\n",
    "\n",
    "    new_row = {\n",
    "        \"model_name\": model_name,\n",
    "        \"embedding_model\":embedding_model_name,\n",
    "        \"n_neighbors\":n_neighbors,\n",
    "        \"n_components\":n_components,\n",
    "        \"min_cluster_size\":min_cluster_size,\n",
    "        \"train_time\": train_time,\n",
    "        \"monogram_cv\": monogram_cv,\n",
    "        \"multigram_cv\": multigram_cv,\n",
    "        \"eval_time\": eval_time\n",
    "    }\n",
    "\n",
    "    evaluation_log_df = pd.concat([evaluation_log_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    evaluation_log_df.to_csv(MODEL_EVALUATION_LOG, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4125b8-9e2d-4bd9-a3d8-c3d1ca8d7c06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Result (Best Model) - WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ee959-9d1c-423b-9c3d-5ce4a70de563",
   "metadata": {},
   "source": [
    "### Topic Info (Monogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "079e19e6-7e0c-4104-adb3-d79e6b736c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Make sure to either pass a valid directory or HF model.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m monogram_topic_model = \u001b[43mBERTopic\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m monogram_topic_model.get_topic_info()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\URECA\\venv\\Lib\\site-packages\\bertopic\\_bertopic.py:3430\u001b[39m, in \u001b[36mBERTopic.load\u001b[39m\u001b[34m(cls, path, embedding_model)\u001b[39m\n\u001b[32m   3428\u001b[39m     topics, params, tensors, ctfidf_tensors, ctfidf_config, images = save_utils.load_files_from_hf(path)\n\u001b[32m   3429\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMake sure to either pass a valid directory or HF model.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3431\u001b[39m topic_model = _create_model_from_files(\n\u001b[32m   3432\u001b[39m     topics,\n\u001b[32m   3433\u001b[39m     params,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3438\u001b[39m     warn_no_backend=(embedding_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   3439\u001b[39m )\n\u001b[32m   3441\u001b[39m \u001b[38;5;66;03m# Replace embedding model if one is specifically chosen\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Make sure to either pass a valid directory or HF model."
     ]
    }
   ],
   "source": [
    "monogram_topic_model = BERTopic.load(\"topic_model\",embedding_model=embedding_model)\n",
    "monogram_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc36926-f73b-4e6d-94f2-cb9f434236e1",
   "metadata": {},
   "source": [
    "### Topic Info (Multigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663dcec9-f8f7-492d-9535-37e9c202853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multigram_topic_model = BERTopic.load(\"topic_model\",embedding_model=embedding_model)\n",
    "multigram_topic_model.update_topics(texts_clean, vectorizer_model=CountVectorizer(stop_words=\"english\", ngram_range=(2,3)))\n",
    "multigram_topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3afba-bedb-4e22-a531-148e71e23202",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a63bc3-db86-4f32-8ee4-2de13c4c4321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Tokenize Document\n",
    "tokenized_texts = [[str(token) for token in doc.split() if token.strip() != ''] for doc in texts_clean]\n",
    "\n",
    "# Create Dictionary\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Extract Topics\n",
    "# Filter topic words to exist in the dictionary\n",
    "topics = [\n",
    "    [str(word) for word, _ in words_probs if str(word) in dictionary.token2id]\n",
    "    for topic_id, words_probs in monogram_topic_model.get_topics().items()\n",
    "    if topic_id != -1\n",
    "]\n",
    "\n",
    "# Remove empty topics (just in case)\n",
    "topics = [t for t in topics if len(t) > 0]\n",
    "\n",
    "# Compute Coherence\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "monogram_coherence = coherence_model.get_coherence()\n",
    "print(\"Monogram C_v Coherence:\", monogram_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7e1f9f-b717-42ef-8bf7-c6835d9beebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts = [doc.split() for doc in texts_clean]\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "\n",
    "# Topics have to be split into singular words\n",
    "topics = [\n",
    "    sum([word.split() for word, _ in multigram_topic_model.get_topic(topic)], [])\n",
    "    for topic in multigram_topic_model.get_topics().keys()\n",
    "    if topic != -1\n",
    "]\n",
    "\n",
    "# Remove empty topics (just in case)\n",
    "topics = [t for t in topics if len(t) > 0]\n",
    "\n",
    "coherence_model = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v'\n",
    ")\n",
    "\n",
    "multigram_coherence = coherence_model.get_coherence()\n",
    "print(\"Multigram C_v Coherence:\", multigram_coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8d8dd-4466-42b9-bc84-b0d62d2bec92",
   "metadata": {},
   "source": [
    "## 7. Using LLM to Improve Representation (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636ab163-622c-44e3-94df-530c09acc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from bertopic.representation import OpenAI\n",
    "\n",
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "topic_model.update_topics(texts_clean, representation_model=OpenAI(client, model=\"gpt-4o-mini\", delay_in_seconds=3))\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0344cd0-ca4e-41cd-825d-c31ac30c24c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
